{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014947,
     "end_time": "2020-12-20T07:00:17.434220",
     "exception": false,
     "start_time": "2020-12-20T07:00:17.419273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I want to convey two things in this notebook.\n",
    "## 1. Don't have to be hesitant about using Loop.\n",
    "They say \"avoid loops!'.\n",
    "But I think It's not bad idea to use loops for this competition.\n",
    "Because:\n",
    "* We have to use small batch inference using Time-series API.\n",
    "* Loops have very small overhead for each batch.\n",
    "* Loops are more flexible.\n",
    "* Even loops are not so slow. 3 features are extracted within 10 minits for 100M train data, as you can see blow.\n",
    "\n",
    "## 2. Future information should not be used.\n",
    "Time-series API doesn't allow us to use information from the future.\n",
    "So we should not use it, especially user statistics from future make things very bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T07:00:17.466983Z",
     "iopub.status.busy": "2020-12-20T07:00:17.465852Z",
     "iopub.status.idle": "2020-12-20T07:00:18.760351Z",
     "shell.execute_reply": "2020-12-20T07:00:18.760949Z"
    },
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 1.312214,
     "end_time": "2020-12-20T07:00:18.761185",
     "exception": false,
     "start_time": "2020-12-20T07:00:17.448971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict, deque\n",
    "from tqdm.notebook import tqdm\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013056,
     "end_time": "2020-12-20T07:00:18.787977",
     "exception": false,
     "start_time": "2020-12-20T07:00:18.774921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## setting\n",
    "CV files are generated by [this notebook](https://www.kaggle.com/its7171/cv-strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T07:00:18.818965Z",
     "iopub.status.busy": "2020-12-20T07:00:18.817906Z",
     "iopub.status.idle": "2020-12-20T07:00:18.824192Z",
     "shell.execute_reply": "2020-12-20T07:00:18.823404Z"
    },
    "papermill": {
     "duration": 0.022992,
     "end_time": "2020-12-20T07:00:18.824324",
     "exception": false,
     "start_time": "2020-12-20T07:00:18.801332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_pickle = '../input/riiid-cross-validation-files/cv1_train.pickle'\n",
    "valid_pickle = '../input/riiid-cross-validation-files/cv1_valid.pickle'\n",
    "question_file = '../input/features/question3.csv'\n",
    "debug = False\n",
    "validaten_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013228,
     "end_time": "2020-12-20T07:00:18.851238",
     "exception": false,
     "start_time": "2020-12-20T07:00:18.838010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T07:00:18.895843Z",
     "iopub.status.busy": "2020-12-20T07:00:18.892833Z",
     "iopub.status.idle": "2020-12-20T07:00:35.304318Z",
     "shell.execute_reply": "2020-12-20T07:00:35.302732Z"
    },
    "papermill": {
     "duration": 16.439708,
     "end_time": "2020-12-20T07:00:35.304466",
     "exception": false,
     "start_time": "2020-12-20T07:00:18.864758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/train_current_part.pkl\", \"rb\")\n",
    "train_current_part = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/train_current_question.pkl\", \"rb\")\n",
    "train_current_question = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/train_current_tag.pkl\", \"rb\")\n",
    "train_current_tag = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/train_other_feats.pkl\", \"rb\")\n",
    "train_other_feats = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/train_past_part_answ.pkl\", \"rb\")\n",
    "train_past_part_answ = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/train_past_quest_answ.pkl\", \"rb\")\n",
    "train_past_quest_answ = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/train_past_tag_answ.pkl\", \"rb\")\n",
    "train_past_tag_answ = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/train_y.pkl\", \"rb\")\n",
    "train_y = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/train_prior_exp.pkl\", \"rb\")\n",
    "train_prior_exp = pickle.load(loaded_pickle)\n",
    "\n",
    "\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/valid_current_part.pkl\", \"rb\")\n",
    "valid_current_part = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/valid_current_question.pkl\", \"rb\")\n",
    "valid_current_question = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/valid_current_tag.pkl\", \"rb\")\n",
    "valid_current_tag = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/valid_other_feats.pkl\", \"rb\")\n",
    "valid_other_feats = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/valid_past_part_answ.pkl\", \"rb\")\n",
    "valid_past_part_answ = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/valid_past_quest_answ.pkl\", \"rb\")\n",
    "valid_past_quest_answ = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/valid_past_tag_answ.pkl\", \"rb\")\n",
    "valid_past_tag_answ = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/valid_y.pkl\", \"rb\")\n",
    "valid_y = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/valid_prior_exp.pkl\", \"rb\")\n",
    "valid_prior_exp = pickle.load(loaded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T07:00:35.346779Z",
     "iopub.status.busy": "2020-12-20T07:00:35.345914Z",
     "iopub.status.idle": "2020-12-20T07:00:42.605844Z",
     "shell.execute_reply": "2020-12-20T07:00:42.605132Z"
    },
    "papermill": {
     "duration": 7.286665,
     "end_time": "2020-12-20T07:00:42.606002",
     "exception": false,
     "start_time": "2020-12-20T07:00:35.319337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, LSTM, Masking, Embedding, Concatenate, Input, Reshape,Flatten, AveragePooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Lambda\n",
    "#from tensorflow.keras.layers import merge\n",
    "from tensorflow.keras.layers import multiply, Reshape\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013733,
     "end_time": "2020-12-20T07:00:42.634109",
     "exception": false,
     "start_time": "2020-12-20T07:00:42.620376",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T07:00:42.707271Z",
     "iopub.status.busy": "2020-12-20T07:00:42.667800Z",
     "iopub.status.idle": "2020-12-20T07:00:42.731315Z",
     "shell.execute_reply": "2020-12-20T07:00:42.730551Z"
    },
    "papermill": {
     "duration": 0.082983,
     "end_time": "2020-12-20T07:00:42.731454",
     "exception": false,
     "start_time": "2020-12-20T07:00:42.648471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "      q: query shape == (..., seq_len_q, depth)\n",
    "      k: key shape == (..., seq_len_k, depth)\n",
    "      v: value shape == (..., seq_len_v, depth_v)\n",
    "      mask: Float tensor with shape broadcastable \n",
    "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "      output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                       (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.2):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, e, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, e, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(e + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                   maximum_position_encoding, rate=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, e, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        #x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, e, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, en_d_model, en_num_heads, dff, pe_input, rate=0.2):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, en_d_model, en_num_heads, dff, \n",
    "                           pe_input, rate)\n",
    "\n",
    "\n",
    "        self.second_final_layer = tf.keras.layers.Dense(dff)\n",
    "        self.final_layer = Dense(1,activation = 'sigmoid')\n",
    "    \n",
    "    def call(self, inp1, inp2, training, mask):\n",
    "\n",
    "        enc_output = self.encoder(inp1, inp2, training, mask)  # (batch_size, inp_seq_len, d_model)\n",
    "            \n",
    "        second_final_output = self.second_final_layer(enc_output)  # (batch_size, tar_seq_len, question_answer_pair_size)\n",
    "        final_output = self.final_layer(second_final_output)\n",
    "        return final_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T07:00:42.768144Z",
     "iopub.status.busy": "2020-12-20T07:00:42.767208Z",
     "iopub.status.idle": "2020-12-20T07:00:42.772232Z",
     "shell.execute_reply": "2020-12-20T07:00:42.771522Z"
    },
    "papermill": {
     "duration": 0.026337,
     "end_time": "2020-12-20T07:00:42.772354",
     "exception": false,
     "start_time": "2020-12-20T07:00:42.746017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(622657, 60, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_other_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T07:00:42.845553Z",
     "iopub.status.busy": "2020-12-20T07:00:42.844382Z",
     "iopub.status.idle": "2020-12-20T07:00:42.846741Z",
     "shell.execute_reply": "2020-12-20T07:00:42.847576Z"
    },
    "papermill": {
     "duration": 0.037737,
     "end_time": "2020-12-20T07:00:42.847779",
     "exception": false,
     "start_time": "2020-12-20T07:00:42.810042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_other_feats1 = np.reshape(train_other_feats[:,:,0],(-1,60,1))\n",
    "train_other_feats2 = np.reshape(train_other_feats[:,:,1],(-1,60,1))\n",
    "\n",
    "valid_other_feats1 = np.reshape(valid_other_feats[:,:,0],(-1,60,1))\n",
    "valid_other_feats2 = np.reshape(valid_other_feats[:,:,1],(-1,60,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T07:00:42.913728Z",
     "iopub.status.busy": "2020-12-20T07:00:42.910975Z",
     "iopub.status.idle": "2020-12-20T07:00:47.291227Z",
     "shell.execute_reply": "2020-12-20T07:00:47.290559Z"
    },
    "papermill": {
     "duration": 4.419596,
     "end_time": "2020-12-20T07:00:47.291345",
     "exception": false,
     "start_time": "2020-12-20T07:00:42.871749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# detect and init the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\n",
    "# instantiate a distribution strategy\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T07:00:47.332299Z",
     "iopub.status.busy": "2020-12-20T07:00:47.331164Z",
     "iopub.status.idle": "2020-12-20T07:00:47.352262Z",
     "shell.execute_reply": "2020-12-20T07:00:47.351623Z"
    },
    "papermill": {
     "duration": 0.045821,
     "end_time": "2020-12-20T07:00:47.352403",
     "exception": false,
     "start_time": "2020-12-20T07:00:47.306582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "dff = 512\n",
    "question_answer_pair_size = 27048\n",
    "tag_answer_pair_size = 3042\n",
    "part_answer_pair_size = 16\n",
    "n_question = 13524\n",
    "n_tag = 1521\n",
    "n_part = 8\n",
    "n_prev_exp = 4\n",
    "\n",
    "pe_input = 60\n",
    "\n",
    "\n",
    "\n",
    "def build(num_layers, d_model, num_heads, dff, question_answer_pair_size, tag_answer_pair_size, part_answer_pair_size\n",
    "          , n_question, n_tag, n_part, n_prev_exp, pe_input):\n",
    "    masking_func = lambda inputs, previous_mask: previous_mask\n",
    "    en_input1 = Input(batch_shape = (None, None), name = 'question_answer_pair')\n",
    "    en_input1_embed = Embedding(question_answer_pair_size, d_model)(en_input1)\n",
    "    en_input2 = Input(batch_shape = (None, None), name = 'tag_answer_pair')\n",
    "    en_input2_embed = Embedding(tag_answer_pair_size, d_model)(en_input2)\n",
    "    en_input3 = Input(batch_shape = (None, None), name = 'part_answer_pair')\n",
    "    en_input3_embed = Embedding(part_answer_pair_size, d_model)(en_input3)\n",
    "    en_input7 = Input(batch_shape = (None, None, 1), name = 'other_feature1')\n",
    "    en_input7_masked = (Masking(mask_value= 0, input_shape = (None, None, 1)))(en_input7)\n",
    "    en_input7_embed = Dense(d_model, input_shape = (None, None, 1))(en_input7_masked)\n",
    "    en_input8 = Input(batch_shape = (None, None, 1), name = 'other_feature2')\n",
    "    en_input8_masked = (Masking(mask_value= 0, input_shape = (None, None, 1)))(en_input8)\n",
    "    en_input8_embed = Dense(d_model, input_shape = (None, None, 1))(en_input8_masked)\n",
    "    en_input9 = Input(batch_shape = (None, None), name = 'prev_q_exp')\n",
    "    en_input9_embed = Embedding(n_prev_exp, d_model)(en_input9)\n",
    "    \n",
    "    \n",
    "    en_input_embed_sum = tf.math.add_n([en_input1_embed, en_input2_embed, en_input3_embed, en_input7_embed, en_input8_embed, en_input9_embed])\n",
    "    \n",
    "    #en_input1_embed = K.sum(en_input1_embed, axis = -2)\n",
    "    en_input4 = Input(batch_shape = (None, None), name = 'current_question')\n",
    "    en_input4_embed = Embedding(n_question, d_model)(en_input4)\n",
    "    en_input5 = Input(batch_shape = (None, None), name = 'current_tag')\n",
    "    en_input5_embed = Embedding(n_tag, d_model)(en_input5)\n",
    "    en_input6 = Input(batch_shape = (None, None), name = 'current_part')\n",
    "    en_input6_embed = Embedding(n_part, d_model)(en_input6)\n",
    "    en_input_embed_sum2 = tf.math.add_n([en_input4_embed, en_input5_embed, en_input6_embed])\n",
    "    \n",
    "    \n",
    "    #en_input2_embed = K.sum(en_input2_embed, axis = -2)\n",
    "\n",
    "    \n",
    "    \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(en_input_embed_sum)[1])\n",
    "    padding_mask = create_padding_mask(en_input1)\n",
    "    combined_mask = tf.maximum(look_ahead_mask, padding_mask)\n",
    "    \n",
    "    \n",
    "    transformer = Transformer(num_layers, d_model, num_heads, dff, pe_input)\n",
    "    \n",
    "    final_output = transformer(en_input_embed_sum, en_input_embed_sum2, True, combined_mask)\n",
    "\n",
    "    \n",
    "    model = Model(inputs=[en_input1, en_input2, en_input3, en_input4, en_input5, en_input6, en_input7, en_input8, en_input9], outputs=final_output)\n",
    "    #model.compile( optimizer = 'adam',\n",
    "    #                loss = 'binary_crossentropy',\n",
    "    #                metrics=['accuracy',AUC()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#my_model = build(num_layers, d_model, num_heads, dff, question_answer_pair_size, tag_answer_pair_size, part_answer_pair_size\n",
    "#          , n_question, n_tag, n_part, n_prev_exp, pe_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T07:00:47.390805Z",
     "iopub.status.busy": "2020-12-20T07:00:47.390159Z",
     "iopub.status.idle": "2020-12-20T07:00:52.542495Z",
     "shell.execute_reply": "2020-12-20T07:00:52.543112Z"
    },
    "papermill": {
     "duration": 5.175259,
     "end_time": "2020-12-20T07:00:52.543270",
     "exception": false,
     "start_time": "2020-12-20T07:00:47.368011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tpu_strategy.scope():\n",
    "    my_model = build(num_layers, d_model, num_heads, dff, question_answer_pair_size, tag_answer_pair_size, part_answer_pair_size\n",
    "                      , n_question, n_tag, n_part, n_prev_exp, pe_input)\n",
    "    my_model.compile( optimizer = 'adam',\n",
    "                        loss = 'binary_crossentropy',\n",
    "                        metrics=['accuracy',AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T07:00:54.572201Z",
     "iopub.status.busy": "2020-12-20T07:00:53.735262Z",
     "iopub.status.idle": "2020-12-20T07:36:13.313790Z",
     "shell.execute_reply": "2020-12-20T07:36:13.312706Z"
    },
    "papermill": {
     "duration": 2120.754239,
     "end_time": "2020-12-20T07:36:13.313982",
     "exception": false,
     "start_time": "2020-12-20T07:00:52.559743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3114/3114 [==============================] - ETA: 0s - auc: 0.7690 - accuracy: 0.7223 - loss: 0.5313\n",
      "Epoch 00001: val_auc improved from -inf to 0.82537, saving model to Transformer_model_feature_extraction.h5\n",
      "3114/3114 [==============================] - 123s 39ms/step - auc: 0.7690 - accuracy: 0.7223 - loss: 0.5313 - val_auc: 0.8254 - val_accuracy: 0.7457 - val_loss: 0.4858\n",
      "Epoch 2/100\n",
      "3113/3114 [============================>.] - ETA: 0s - auc: 0.7961 - accuracy: 0.7379 - loss: 0.5084\n",
      "Epoch 00002: val_auc improved from 0.82537 to 0.83811, saving model to Transformer_model_feature_extraction.h5\n",
      "3114/3114 [==============================] - 109s 35ms/step - auc: 0.7961 - accuracy: 0.7379 - loss: 0.5084 - val_auc: 0.8381 - val_accuracy: 0.7601 - val_loss: 0.4665\n",
      "Epoch 3/100\n",
      "3114/3114 [==============================] - ETA: 0s - auc: 0.8039 - accuracy: 0.7427 - loss: 0.5015\n",
      "Epoch 00003: val_auc improved from 0.83811 to 0.83979, saving model to Transformer_model_feature_extraction.h5\n",
      "3114/3114 [==============================] - 109s 35ms/step - auc: 0.8039 - accuracy: 0.7427 - loss: 0.5015 - val_auc: 0.8398 - val_accuracy: 0.7617 - val_loss: 0.4646\n",
      "Epoch 4/100\n",
      "3113/3114 [============================>.] - ETA: 0s - auc: 0.8060 - accuracy: 0.7441 - loss: 0.4995\n",
      "Epoch 00004: val_auc improved from 0.83979 to 0.84008, saving model to Transformer_model_feature_extraction.h5\n",
      "3114/3114 [==============================] - 109s 35ms/step - auc: 0.8060 - accuracy: 0.7441 - loss: 0.4995 - val_auc: 0.8401 - val_accuracy: 0.7620 - val_loss: 0.4644\n",
      "Epoch 5/100\n",
      "3114/3114 [==============================] - ETA: 0s - auc: 0.8073 - accuracy: 0.7450 - loss: 0.4982\n",
      "Epoch 00005: val_auc improved from 0.84008 to 0.84081, saving model to Transformer_model_feature_extraction.h5\n",
      "3114/3114 [==============================] - 110s 35ms/step - auc: 0.8073 - accuracy: 0.7450 - loss: 0.4982 - val_auc: 0.8408 - val_accuracy: 0.7625 - val_loss: 0.4635\n",
      "Epoch 6/100\n",
      "3113/3114 [============================>.] - ETA: 0s - auc: 0.8082 - accuracy: 0.7457 - loss: 0.4973\n",
      "Epoch 00006: val_auc improved from 0.84081 to 0.84092, saving model to Transformer_model_feature_extraction.h5\n",
      "3114/3114 [==============================] - 109s 35ms/step - auc: 0.8082 - accuracy: 0.7457 - loss: 0.4973 - val_auc: 0.8409 - val_accuracy: 0.7618 - val_loss: 0.4641\n",
      "Epoch 7/100\n",
      "3114/3114 [==============================] - ETA: 0s - auc: 0.8090 - accuracy: 0.7462 - loss: 0.4965\n",
      "Epoch 00007: val_auc improved from 0.84092 to 0.84117, saving model to Transformer_model_feature_extraction.h5\n",
      "3114/3114 [==============================] - 109s 35ms/step - auc: 0.8090 - accuracy: 0.7462 - loss: 0.4965 - val_auc: 0.8412 - val_accuracy: 0.7629 - val_loss: 0.4631\n",
      "Epoch 8/100\n",
      "3114/3114 [==============================] - ETA: 0s - auc: 0.8096 - accuracy: 0.7466 - loss: 0.4959\n",
      "Epoch 00008: val_auc did not improve from 0.84117\n",
      "3114/3114 [==============================] - 109s 35ms/step - auc: 0.8096 - accuracy: 0.7466 - loss: 0.4959 - val_auc: 0.8408 - val_accuracy: 0.7627 - val_loss: 0.4635\n",
      "Epoch 9/100\n",
      "3114/3114 [==============================] - ETA: 0s - auc: 0.8102 - accuracy: 0.7470 - loss: 0.4954\n",
      "Epoch 00009: val_auc improved from 0.84117 to 0.84118, saving model to Transformer_model_feature_extraction.h5\n",
      "3114/3114 [==============================] - 110s 35ms/step - auc: 0.8102 - accuracy: 0.7470 - loss: 0.4954 - val_auc: 0.8412 - val_accuracy: 0.7630 - val_loss: 0.4631\n",
      "Epoch 10/100\n",
      "3113/3114 [============================>.] - ETA: 0s - auc: 0.8106 - accuracy: 0.7472 - loss: 0.4950\n",
      "Epoch 00010: val_auc did not improve from 0.84118\n",
      "3114/3114 [==============================] - 108s 35ms/step - auc: 0.8106 - accuracy: 0.7472 - loss: 0.4950 - val_auc: 0.8410 - val_accuracy: 0.7630 - val_loss: 0.4632\n",
      "Epoch 11/100\n",
      "3113/3114 [============================>.] - ETA: 0s - auc: 0.8112 - accuracy: 0.7476 - loss: 0.4944\n",
      "Epoch 00011: val_auc did not improve from 0.84118\n",
      "3114/3114 [==============================] - 108s 35ms/step - auc: 0.8112 - accuracy: 0.7476 - loss: 0.4944 - val_auc: 0.8407 - val_accuracy: 0.7623 - val_loss: 0.4639\n",
      "Epoch 12/100\n",
      "3113/3114 [============================>.] - ETA: 0s - auc: 0.8116 - accuracy: 0.7478 - loss: 0.4940\n",
      "Epoch 00012: val_auc did not improve from 0.84118\n",
      "3114/3114 [==============================] - 109s 35ms/step - auc: 0.8116 - accuracy: 0.7478 - loss: 0.4940 - val_auc: 0.8409 - val_accuracy: 0.7629 - val_loss: 0.4633\n",
      "Epoch 13/100\n",
      "3114/3114 [==============================] - ETA: 0s - auc: 0.8121 - accuracy: 0.7481 - loss: 0.4935\n",
      "Epoch 00013: val_auc did not improve from 0.84118\n",
      "3114/3114 [==============================] - 108s 35ms/step - auc: 0.8121 - accuracy: 0.7481 - loss: 0.4935 - val_auc: 0.8402 - val_accuracy: 0.7619 - val_loss: 0.4645\n",
      "Epoch 14/100\n",
      "3113/3114 [============================>.] - ETA: 0s - auc: 0.8122 - accuracy: 0.7483 - loss: 0.4933\n",
      "Epoch 00014: val_auc did not improve from 0.84118\n",
      "3114/3114 [==============================] - 110s 35ms/step - auc: 0.8122 - accuracy: 0.7483 - loss: 0.4933 - val_auc: 0.8403 - val_accuracy: 0.7625 - val_loss: 0.4639\n",
      "Epoch 15/100\n",
      "3114/3114 [==============================] - ETA: 0s - auc: 0.8127 - accuracy: 0.7486 - loss: 0.4929\n",
      "Epoch 00015: val_auc did not improve from 0.84118\n",
      "3114/3114 [==============================] - 108s 35ms/step - auc: 0.8127 - accuracy: 0.7486 - loss: 0.4929 - val_auc: 0.8399 - val_accuracy: 0.7621 - val_loss: 0.4651\n",
      "Epoch 16/100\n",
      "3114/3114 [==============================] - ETA: 0s - auc: 0.8131 - accuracy: 0.7489 - loss: 0.4925\n",
      "Epoch 00016: val_auc did not improve from 0.84118\n",
      "3114/3114 [==============================] - 109s 35ms/step - auc: 0.8131 - accuracy: 0.7489 - loss: 0.4925 - val_auc: 0.8401 - val_accuracy: 0.7623 - val_loss: 0.4649\n",
      "Epoch 17/100\n",
      "3113/3114 [============================>.] - ETA: 0s - auc: 0.8134 - accuracy: 0.7491 - loss: 0.4921\n",
      "Epoch 00017: val_auc did not improve from 0.84118\n",
      "3114/3114 [==============================] - 109s 35ms/step - auc: 0.8134 - accuracy: 0.7491 - loss: 0.4921 - val_auc: 0.8399 - val_accuracy: 0.7622 - val_loss: 0.4647\n",
      "Epoch 18/100\n",
      "3114/3114 [==============================] - ETA: 0s - auc: 0.8137 - accuracy: 0.7493 - loss: 0.4918\n",
      "Epoch 00018: val_auc did not improve from 0.84118\n",
      "3114/3114 [==============================] - 109s 35ms/step - auc: 0.8137 - accuracy: 0.7493 - loss: 0.4918 - val_auc: 0.8401 - val_accuracy: 0.7621 - val_loss: 0.4646\n",
      "Epoch 19/100\n",
      "3113/3114 [============================>.] - ETA: 0s - auc: 0.8140 - accuracy: 0.7494 - loss: 0.4915\n",
      "Epoch 00019: val_auc did not improve from 0.84118\n",
      "3114/3114 [==============================] - 109s 35ms/step - auc: 0.8140 - accuracy: 0.7494 - loss: 0.4915 - val_auc: 0.8397 - val_accuracy: 0.7620 - val_loss: 0.4646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4c355ab650>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=10, mode='max', restore_best_weights=True) \n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath='Transformer_model_feature_extraction.h5', monitor='val_auc', mode='max', save_best_only=True, save_weights_only=True, verbose=1)\n",
    "\n",
    "my_model.fit([train_past_quest_answ, train_past_tag_answ, train_past_part_answ, \n",
    "                               train_current_question, train_current_tag, train_current_part,\n",
    "                               train_other_feats1,train_other_feats2, train_prior_exp],train_y, \n",
    "             validation_data=([valid_past_quest_answ, valid_past_tag_answ, valid_past_part_answ,\n",
    "                               valid_current_question, valid_current_tag, valid_current_part,\n",
    "                               valid_other_feats1, valid_other_feats2, valid_prior_exp],valid_y), batch_size = 200,\n",
    "             epochs = 100, verbose = 1, callbacks = [earlyStopping, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T07:36:33.734429Z",
     "iopub.status.busy": "2020-12-20T07:36:33.733360Z",
     "iopub.status.idle": "2020-12-20T07:36:33.737127Z",
     "shell.execute_reply": "2020-12-20T07:36:33.736556Z"
    },
    "papermill": {
     "duration": 10.21636,
     "end_time": "2020-12-20T07:36:33.737251",
     "exception": false,
     "start_time": "2020-12-20T07:36:23.520891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#my_model.save_weights('Transformer_model_feature_extraction.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 10.237968,
     "end_time": "2020-12-20T07:36:54.104533",
     "exception": false,
     "start_time": "2020-12-20T07:36:43.866565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Have a fun with loops! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T07:37:14.667079Z",
     "iopub.status.busy": "2020-12-20T07:37:14.666400Z",
     "iopub.status.idle": "2020-12-20T07:37:14.669511Z",
     "shell.execute_reply": "2020-12-20T07:37:14.668857Z"
    },
    "papermill": {
     "duration": 10.296099,
     "end_time": "2020-12-20T07:37:14.669631",
     "exception": false,
     "start_time": "2020-12-20T07:37:04.373532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#my_model.load_weights('../input/transformer-model/Transformer_model_feature_extraction.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T07:37:35.016046Z",
     "iopub.status.busy": "2020-12-20T07:37:35.015033Z",
     "iopub.status.idle": "2020-12-20T07:37:46.875038Z",
     "shell.execute_reply": "2020-12-20T07:37:46.875607Z"
    },
    "papermill": {
     "duration": 22.07208,
     "end_time": "2020-12-20T07:37:46.875778",
     "exception": false,
     "start_time": "2020-12-20T07:37:24.803698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_hat = my_model.predict([valid_past_quest_answ, valid_past_tag_answ, valid_past_part_answ,\n",
    "                               valid_current_question, valid_current_tag, valid_current_part,\n",
    "                               valid_other_feats1, valid_other_feats2, valid_prior_exp], batch_size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T07:38:07.647108Z",
     "iopub.status.busy": "2020-12-20T07:38:07.646284Z",
     "iopub.status.idle": "2020-12-20T07:38:07.675324Z",
     "shell.execute_reply": "2020-12-20T07:38:07.675917Z"
    },
    "papermill": {
     "duration": 10.524928,
     "end_time": "2020-12-20T07:38:07.676076",
     "exception": false,
     "start_time": "2020-12-20T07:37:57.151148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7656748294398391\n"
     ]
    }
   ],
   "source": [
    "y_hat = y_hat[:,-1,0]\n",
    "y = valid_y[:,-1,0]\n",
    "print(roc_auc_score(y,y_hat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 2286.014113,
   "end_time": "2020-12-20T07:38:18.200631",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-20T07:00:12.186518",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
