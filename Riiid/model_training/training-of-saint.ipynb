{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015355,
     "end_time": "2020-12-12T06:02:46.269818",
     "exception": false,
     "start_time": "2020-12-12T06:02:46.254463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I want to convey two things in this notebook.\n",
    "## 1. Don't have to be hesitant about using Loop.\n",
    "They say \"avoid loops!'.\n",
    "But I think It's not bad idea to use loops for this competition.\n",
    "Because:\n",
    "* We have to use small batch inference using Time-series API.\n",
    "* Loops have very small overhead for each batch.\n",
    "* Loops are more flexible.\n",
    "* Even loops are not so slow. 3 features are extracted within 10 minits for 100M train data, as you can see blow.\n",
    "\n",
    "## 2. Future information should not be used.\n",
    "Time-series API doesn't allow us to use information from the future.\n",
    "So we should not use it, especially user statistics from future make things very bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-12T06:02:46.302319Z",
     "iopub.status.busy": "2020-12-12T06:02:46.301552Z",
     "iopub.status.idle": "2020-12-12T06:02:47.271370Z",
     "shell.execute_reply": "2020-12-12T06:02:47.270120Z"
    },
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 0.989117,
     "end_time": "2020-12-12T06:02:47.271550",
     "exception": false,
     "start_time": "2020-12-12T06:02:46.282433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict, deque\n",
    "from tqdm.notebook import tqdm\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012995,
     "end_time": "2020-12-12T06:02:47.296750",
     "exception": false,
     "start_time": "2020-12-12T06:02:47.283755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## setting\n",
    "CV files are generated by [this notebook](https://www.kaggle.com/its7171/cv-strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-12T06:02:47.325604Z",
     "iopub.status.busy": "2020-12-12T06:02:47.324729Z",
     "iopub.status.idle": "2020-12-12T06:02:47.327860Z",
     "shell.execute_reply": "2020-12-12T06:02:47.327179Z"
    },
    "papermill": {
     "duration": 0.019066,
     "end_time": "2020-12-12T06:02:47.327971",
     "exception": false,
     "start_time": "2020-12-12T06:02:47.308905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_pickle = '../input/riiid-cross-validation-files/cv1_train.pickle'\n",
    "valid_pickle = '../input/riiid-cross-validation-files/cv1_valid.pickle'\n",
    "question_file = '../input/features/question3.csv'\n",
    "debug = False\n",
    "validaten_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012406,
     "end_time": "2020-12-12T06:02:47.352609",
     "exception": false,
     "start_time": "2020-12-12T06:02:47.340203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-12T06:02:47.391353Z",
     "iopub.status.busy": "2020-12-12T06:02:47.390596Z",
     "iopub.status.idle": "2020-12-12T06:02:59.370571Z",
     "shell.execute_reply": "2020-12-12T06:02:59.369438Z"
    },
    "papermill": {
     "duration": 12.006502,
     "end_time": "2020-12-12T06:02:59.370725",
     "exception": false,
     "start_time": "2020-12-12T06:02:47.364223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/train_current_part.pkl\", \"rb\")\n",
    "train_current_part = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/train_current_question.pkl\", \"rb\")\n",
    "train_current_question = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/train_current_tag.pkl\", \"rb\")\n",
    "train_current_tag = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/train_other_feats.pkl\", \"rb\")\n",
    "train_other_feats = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/train_past_part_answ.pkl\", \"rb\")\n",
    "train_past_part_answ = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/train_past_quest_answ.pkl\", \"rb\")\n",
    "train_past_quest_answ = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/train_past_tag_answ.pkl\", \"rb\")\n",
    "train_past_tag_answ = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/train_y.pkl\", \"rb\")\n",
    "train_y = pickle.load(loaded_pickle)\n",
    "\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/valid_current_part.pkl\", \"rb\")\n",
    "valid_current_part = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/valid_current_question.pkl\", \"rb\")\n",
    "valid_current_question = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/valid_current_tag.pkl\", \"rb\")\n",
    "valid_current_tag = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/valid_other_feats.pkl\", \"rb\")\n",
    "valid_other_feats = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/valid_past_part_answ.pkl\", \"rb\")\n",
    "valid_past_part_answ = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/valid_past_quest_answ.pkl\", \"rb\")\n",
    "valid_past_quest_answ = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/valid_past_tag_answ.pkl\", \"rb\")\n",
    "valid_past_tag_answ = pickle.load(loaded_pickle)\n",
    "\n",
    "loaded_pickle = open(\"../input/for-transformer/valid_y.pkl\", \"rb\")\n",
    "valid_y = pickle.load(loaded_pickle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-12T06:02:59.406820Z",
     "iopub.status.busy": "2020-12-12T06:02:59.405998Z",
     "iopub.status.idle": "2020-12-12T06:03:04.972149Z",
     "shell.execute_reply": "2020-12-12T06:03:04.970687Z"
    },
    "papermill": {
     "duration": 5.588451,
     "end_time": "2020-12-12T06:03:04.972302",
     "exception": false,
     "start_time": "2020-12-12T06:02:59.383851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, LSTM, Masking, Embedding, Concatenate, Input, Reshape,Flatten, AveragePooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Lambda\n",
    "#from tensorflow.keras.layers import merge\n",
    "from tensorflow.keras.layers import multiply, Reshape\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013403,
     "end_time": "2020-12-12T06:03:04.998677",
     "exception": false,
     "start_time": "2020-12-12T06:03:04.985274",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-12T06:03:05.043364Z",
     "iopub.status.busy": "2020-12-12T06:03:05.032357Z",
     "iopub.status.idle": "2020-12-12T06:03:05.105599Z",
     "shell.execute_reply": "2020-12-12T06:03:05.106085Z"
    },
    "papermill": {
     "duration": 0.095241,
     "end_time": "2020-12-12T06:03:05.106235",
     "exception": false,
     "start_time": "2020-12-12T06:03:05.010994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "      q: query shape == (..., seq_len_q, depth)\n",
    "      k: key shape == (..., seq_len_k, depth)\n",
    "      v: value shape == (..., seq_len_v, depth_v)\n",
    "      mask: Float tensor with shape broadcastable \n",
    "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "      output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                       (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class EncoderLayer2(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.2):\n",
    "        super(EncoderLayer2, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "\n",
    "class Encoder2(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                   maximum_position_encoding, rate=0.2):\n",
    "        super(Encoder2, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer2(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        #x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    \n",
    "class DecoderLayer2(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.2):\n",
    "        super(DecoderLayer2, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, enc_output, training, \n",
    "                look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "                enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "    \n",
    "    \n",
    "class Decoder2(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                    maximum_position_encoding, rate=0.2):\n",
    "        super(Decoder2, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer2(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, \n",
    "               look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        #x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "              x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "        attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "        attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights    \n",
    "\n",
    "class Transformer2(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, padding_length, rate=0.2):\n",
    "        super(Transformer2, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder2(num_layers, d_model, num_heads, dff, padding_length)\n",
    "        \n",
    "        self.decoder = Decoder2(num_layers, d_model, num_heads, dff, padding_length)\n",
    "\n",
    "        self.second_final_layer = tf.keras.layers.Dense(dff)\n",
    "        self.final_layer = Dense(1,activation = 'sigmoid')\n",
    "    \n",
    "    def call(self, inp1, inp2, training, en_combined_mask, de_look_ahead_mask, de_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp1, training, en_combined_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "                inp2, enc_output, training, de_look_ahead_mask, de_padding_mask)\n",
    "            \n",
    "        second_final_output = self.second_final_layer(dec_output)  # (batch_size, tar_seq_len, question_answer_pair_size)\n",
    "        final_output = self.final_layer(second_final_output)\n",
    "        return final_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-12T06:03:05.141210Z",
     "iopub.status.busy": "2020-12-12T06:03:05.140511Z",
     "iopub.status.idle": "2020-12-12T06:03:05.143624Z",
     "shell.execute_reply": "2020-12-12T06:03:05.144086Z"
    },
    "papermill": {
     "duration": 0.024399,
     "end_time": "2020-12-12T06:03:05.144205",
     "exception": false,
     "start_time": "2020-12-12T06:03:05.119806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_other_feats1 = np.reshape(train_other_feats[:,:,0],(-1,60,1))\n",
    "train_other_feats2 = np.reshape(train_other_feats[:,:,1],(-1,60,1))\n",
    "\n",
    "valid_other_feats1 = np.reshape(valid_other_feats[:,:,0],(-1,60,1))\n",
    "valid_other_feats2 = np.reshape(valid_other_feats[:,:,1],(-1,60,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-12T06:03:05.195759Z",
     "iopub.status.busy": "2020-12-12T06:03:05.194990Z",
     "iopub.status.idle": "2020-12-12T06:03:10.640596Z",
     "shell.execute_reply": "2020-12-12T06:03:10.639235Z"
    },
    "papermill": {
     "duration": 5.483455,
     "end_time": "2020-12-12T06:03:10.640761",
     "exception": false,
     "start_time": "2020-12-12T06:03:05.157306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "num_layers = 2\n",
    "d_model = 128\n",
    "num_heads = 2\n",
    "dff = 512\n",
    "\n",
    "question_answer_pair_size = 27048\n",
    "tag_answer_pair_size = 3042\n",
    "part_answer_pair_size = 16\n",
    "n_question = 13524\n",
    "n_tag = 1521\n",
    "n_part = 8\n",
    "\n",
    "pe_input = 60\n",
    "epoch = 10\n",
    "max_len = 100\n",
    "\n",
    "\n",
    "def build(num_layers, d_model, num_heads, dff, n_question, n_tag, n_part, pe_input,\n",
    "          question_answer_pair_size, tag_answer_pair_size, part_answer_pair_size):\n",
    "\n",
    "    en_input1 = Input(batch_shape = (None, None), name = 'current_question')\n",
    "    en_input1_embed = Embedding(n_question, d_model)(en_input1)\n",
    "    en_input2 = Input(batch_shape = (None, None), name = 'current_tag')\n",
    "    en_input2_embed = Embedding(n_tag, d_model)(en_input2)\n",
    "    en_input3 = Input(batch_shape = (None, None), name = 'current_part')\n",
    "    en_input3_embed = Embedding(n_part, d_model)(en_input3)\n",
    "    en_input = tf.math.add_n([en_input1_embed, en_input2_embed, en_input3_embed])\n",
    "\n",
    "    en_look_ahead_mask = create_look_ahead_mask(tf.shape(en_input1)[1])\n",
    "    en_padding_mask = create_padding_mask(en_input1)\n",
    "    en_combined_mask = tf.maximum(en_look_ahead_mask, en_padding_mask)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #en_input1_embed = K.sum(en_input1_embed, axis = -2)\n",
    "    de_input4 = Input(batch_shape = (None, None), name = 'past_question_answer')\n",
    "    de_input4_embed = Embedding(question_answer_pair_size, d_model)(de_input4)\n",
    "    de_input5 = Input(batch_shape = (None, None), name = 'past_tag_answer')\n",
    "    de_input5_embed = Embedding(tag_answer_pair_size, d_model)(de_input5)\n",
    "    de_input6 = Input(batch_shape = (None, None), name = 'past_part_answer')\n",
    "    de_input6_embed = Embedding(part_answer_pair_size, d_model)(de_input6)\n",
    "    de_input7 = Input(batch_shape = (None, None, 1), name = 'other_feature1')\n",
    "    de_input7_masked = (Masking(mask_value= 0, input_shape = (None, None, 1)))(de_input7)\n",
    "    de_input7_embed = Dense(d_model, input_shape = (None, None, 1))(de_input7_masked)\n",
    "    de_input8 = Input(batch_shape = (None, None, 1), name = 'other_feature2')\n",
    "    de_input8_masked = (Masking(mask_value= 0, input_shape = (None, None, 1)))(de_input8)\n",
    "    de_input8_embed = Dense(d_model, input_shape = (None, None, 1))(de_input8_masked)   \n",
    "    de_input = tf.math.add_n([de_input4_embed, de_input5_embed, de_input6_embed, de_input7_embed, de_input8_embed])\n",
    "    \n",
    "    de_look_ahead_mask = create_look_ahead_mask(tf.shape(de_input4)[1])\n",
    "    de_padding_mask = create_padding_mask(de_input4)\n",
    "    de_combined_mask = tf.maximum(de_look_ahead_mask, de_padding_mask)\n",
    "    \n",
    "    \n",
    "    transformer = Transformer2(num_layers, d_model, num_heads, dff, pe_input)\n",
    "    \n",
    "    final_output = transformer(en_input, de_input, True, en_combined_mask, de_combined_mask, de_combined_mask)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[en_input1, en_input2, en_input3, de_input4, de_input5, de_input6, de_input7, de_input8], outputs=final_output)\n",
    "    model.compile( optimizer = 'adam',\n",
    "                    loss = 'binary_crossentropy',\n",
    "                    metrics=['accuracy',AUC()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#my_model = build(num_layers, d_model, num_heads, dff, n_question, n_tag, n_part, n_answer, pe_input, num_other_feats, max_len)\n",
    "\n",
    "\n",
    "my_model = build(num_layers, d_model, num_heads, dff, n_question, n_tag, n_part, pe_input,\n",
    "          question_answer_pair_size, tag_answer_pair_size, part_answer_pair_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-12T06:03:10.679262Z",
     "iopub.status.busy": "2020-12-12T06:03:10.678453Z",
     "iopub.status.idle": "2020-12-12T07:24:38.039919Z",
     "shell.execute_reply": "2020-12-12T07:24:38.039368Z"
    },
    "papermill": {
     "duration": 4887.385268,
     "end_time": "2020-12-12T07:24:38.040035",
     "exception": false,
     "start_time": "2020-12-12T06:03:10.654767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2666/2666 [==============================] - 329s 124ms/step - loss: 0.5494 - accuracy: 0.7128 - auc: 0.7360 - val_loss: 0.4768 - val_accuracy: 0.7543 - val_auc: 0.8287\n",
      "Epoch 2/15\n",
      "2666/2666 [==============================] - 331s 124ms/step - loss: 0.5036 - accuracy: 0.7429 - auc: 0.7999 - val_loss: 0.4648 - val_accuracy: 0.7622 - val_auc: 0.8403\n",
      "Epoch 3/15\n",
      "2666/2666 [==============================] - 334s 125ms/step - loss: 0.4968 - accuracy: 0.7471 - auc: 0.8074 - val_loss: 0.4612 - val_accuracy: 0.7643 - val_auc: 0.8434\n",
      "Epoch 4/15\n",
      "2666/2666 [==============================] - 329s 123ms/step - loss: 0.4925 - accuracy: 0.7495 - auc: 0.8119 - val_loss: 0.4584 - val_accuracy: 0.7660 - val_auc: 0.8462\n",
      "Epoch 5/15\n",
      "2666/2666 [==============================] - 325s 122ms/step - loss: 0.4896 - accuracy: 0.7513 - auc: 0.8151 - val_loss: 0.4570 - val_accuracy: 0.7669 - val_auc: 0.8477\n",
      "Epoch 6/15\n",
      "2666/2666 [==============================] - 329s 123ms/step - loss: 0.4875 - accuracy: 0.7523 - auc: 0.8171 - val_loss: 0.4557 - val_accuracy: 0.7675 - val_auc: 0.8482\n",
      "Epoch 7/15\n",
      "2666/2666 [==============================] - 323s 121ms/step - loss: 0.4857 - accuracy: 0.7534 - auc: 0.8189 - val_loss: 0.4552 - val_accuracy: 0.7675 - val_auc: 0.8486\n",
      "Epoch 8/15\n",
      "2666/2666 [==============================] - 324s 122ms/step - loss: 0.4842 - accuracy: 0.7542 - auc: 0.8204 - val_loss: 0.4551 - val_accuracy: 0.7676 - val_auc: 0.8490\n",
      "Epoch 9/15\n",
      "2666/2666 [==============================] - 321s 120ms/step - loss: 0.4831 - accuracy: 0.7549 - auc: 0.8215 - val_loss: 0.4548 - val_accuracy: 0.7679 - val_auc: 0.8492\n",
      "Epoch 10/15\n",
      "2666/2666 [==============================] - 321s 120ms/step - loss: 0.4821 - accuracy: 0.7554 - auc: 0.8225 - val_loss: 0.4546 - val_accuracy: 0.7681 - val_auc: 0.8496\n",
      "Epoch 11/15\n",
      "2666/2666 [==============================] - 328s 123ms/step - loss: 0.4812 - accuracy: 0.7559 - auc: 0.8233 - val_loss: 0.4546 - val_accuracy: 0.7677 - val_auc: 0.8494\n",
      "Epoch 12/15\n",
      "2666/2666 [==============================] - 322s 121ms/step - loss: 0.4806 - accuracy: 0.7564 - auc: 0.8239 - val_loss: 0.4541 - val_accuracy: 0.7681 - val_auc: 0.8497\n",
      "Epoch 13/15\n",
      "2666/2666 [==============================] - 323s 121ms/step - loss: 0.4799 - accuracy: 0.7567 - auc: 0.8246 - val_loss: 0.4553 - val_accuracy: 0.7669 - val_auc: 0.8496\n",
      "Epoch 14/15\n",
      "2666/2666 [==============================] - 318s 119ms/step - loss: 0.4796 - accuracy: 0.7568 - auc: 0.8249 - val_loss: 0.4542 - val_accuracy: 0.7678 - val_auc: 0.8496\n",
      "Epoch 15/15\n",
      "2666/2666 [==============================] - 319s 120ms/step - loss: 0.4790 - accuracy: 0.7571 - auc: 0.8254 - val_loss: 0.4543 - val_accuracy: 0.7681 - val_auc: 0.8495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb7a841b090>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor=AUC(), patience=2,mode='max') \n",
    "\n",
    "\n",
    "my_model.fit([train_current_question, train_current_tag, train_current_part, \n",
    "              train_past_quest_answ, train_past_tag_answ, train_past_part_answ,\n",
    "                               train_other_feats1, train_other_feats2],train_y, \n",
    "             validation_data=([valid_current_question, valid_current_tag, valid_current_part,\n",
    "                               valid_past_quest_answ, valid_past_tag_answ, valid_past_part_answ,\n",
    "                               valid_other_feats1, valid_other_feats2],valid_y), batch_size = 200,\n",
    "             epochs = 15, verbose = 1, callbacks = [earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-12T07:25:07.877037Z",
     "iopub.status.busy": "2020-12-12T07:25:07.876293Z",
     "iopub.status.idle": "2020-12-12T07:25:08.003081Z",
     "shell.execute_reply": "2020-12-12T07:25:08.001989Z"
    },
    "papermill": {
     "duration": 15.86688,
     "end_time": "2020-12-12T07:25:08.003206",
     "exception": false,
     "start_time": "2020-12-12T07:24:52.136326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_model.save_weights('SAINT_model_feature_extraction.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 14.497951,
     "end_time": "2020-12-12T07:25:36.652240",
     "exception": false,
     "start_time": "2020-12-12T07:25:22.154289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Have a fun with loops! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-12T07:26:06.405815Z",
     "iopub.status.busy": "2020-12-12T07:26:06.405134Z",
     "iopub.status.idle": "2020-12-12T07:26:06.768696Z",
     "shell.execute_reply": "2020-12-12T07:26:06.768184Z"
    },
    "papermill": {
     "duration": 16.201208,
     "end_time": "2020-12-12T07:26:06.768807",
     "exception": false,
     "start_time": "2020-12-12T07:25:50.567599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = '../input/transformer-model/SAINT_model_feature_extraction.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-341b826f8112>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/transformer-model/SAINT_model_feature_extraction.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2202\u001b[0m           'first, then load the weights.')\n\u001b[1;32m   2203\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2204\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2205\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2206\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '../input/transformer-model/SAINT_model_feature_extraction.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "my_model.load_weights('../input/transformer-model/SAINT_model_feature_extraction.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-12T07:26:35.254294Z",
     "iopub.status.busy": "2020-12-12T07:26:35.252453Z",
     "iopub.status.idle": "2020-12-12T07:26:56.120742Z",
     "shell.execute_reply": "2020-12-12T07:26:56.120177Z"
    },
    "papermill": {
     "duration": 35.129077,
     "end_time": "2020-12-12T07:26:56.120863",
     "exception": false,
     "start_time": "2020-12-12T07:26:20.991786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_hat = my_model.predict([valid_current_question, valid_current_tag, valid_current_part,\n",
    "                               valid_past_quest_answ, valid_past_tag_answ, valid_past_part_answ,\n",
    "                               valid_other_feats1, valid_other_feats2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-12T07:27:24.921977Z",
     "iopub.status.busy": "2020-12-12T07:27:24.921194Z",
     "iopub.status.idle": "2020-12-12T07:27:24.946889Z",
     "shell.execute_reply": "2020-12-12T07:27:24.947384Z"
    },
    "papermill": {
     "duration": 13.926099,
     "end_time": "2020-12-12T07:27:24.947555",
     "exception": false,
     "start_time": "2020-12-12T07:27:11.021456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7775014872403258\n"
     ]
    }
   ],
   "source": [
    "y_hat = y_hat[:,-1,0]\n",
    "y = valid_y[:,-1,0]\n",
    "print(roc_auc_score(y,y_hat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 5099.230238,
   "end_time": "2020-12-12T07:27:41.106499",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-12T06:02:41.876261",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
