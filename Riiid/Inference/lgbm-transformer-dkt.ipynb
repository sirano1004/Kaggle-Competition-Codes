{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:38:04.968841Z",
     "iopub.status.busy": "2020-12-27T02:38:04.967920Z",
     "iopub.status.idle": "2020-12-27T02:38:05.899166Z",
     "shell.execute_reply": "2020-12-27T02:38:05.900124Z"
    },
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 0.980777,
     "end_time": "2020-12-27T02:38:05.900390",
     "exception": false,
     "start_time": "2020-12-27T02:38:04.919613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict, deque\n",
    "from tqdm.notebook import tqdm\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035471,
     "end_time": "2020-12-27T02:38:05.977140",
     "exception": false,
     "start_time": "2020-12-27T02:38:05.941669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## setting\n",
    "CV files are generated by [this notebook](https://www.kaggle.com/its7171/cv-strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:38:06.054012Z",
     "iopub.status.busy": "2020-12-27T02:38:06.053274Z",
     "iopub.status.idle": "2020-12-27T02:38:06.055347Z",
     "shell.execute_reply": "2020-12-27T02:38:06.054633Z"
    },
    "papermill": {
     "duration": 0.044236,
     "end_time": "2020-12-27T02:38:06.055468",
     "exception": false,
     "start_time": "2020-12-27T02:38:06.011232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_pickle = '../input/riiid-cross-validation-files/cv1_train.pickle'\n",
    "valid_pickle = '../input/riiid-cross-validation-files/cv1_valid.pickle'\n",
    "question_file = '../input/features/question3.csv'\n",
    "debug = False\n",
    "validaten_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034362,
     "end_time": "2020-12-27T02:38:06.124258",
     "exception": false,
     "start_time": "2020-12-27T02:38:06.089896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:38:06.203782Z",
     "iopub.status.busy": "2020-12-27T02:38:06.202910Z",
     "iopub.status.idle": "2020-12-27T02:38:06.252510Z",
     "shell.execute_reply": "2020-12-27T02:38:06.251632Z"
    },
    "papermill": {
     "duration": 0.091718,
     "end_time": "2020-12-27T02:38:06.252607",
     "exception": false,
     "start_time": "2020-12-27T02:38:06.160889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_user_feats_for_nn_without_update(df, past_question, past_tag, past_part, past_answer, past_prior_elaps, past_time_diff, past_prior_exp):\n",
    "    current_part = []\n",
    "    current_question = []\n",
    "    current_tag = []\n",
    "    past_part_answer = []\n",
    "    past_question_answer = []\n",
    "    past_tag_answer = []\n",
    "    past_answer_correctly = []\n",
    "    past_time = []\n",
    "    past_prior = []\n",
    "    past_prior_explanation = []\n",
    "    #past_other_feats = []\n",
    "    \n",
    "    \n",
    "    for cnt,row in enumerate(tqdm(df[['user_id','content_id','part','tag_num','prior_question_elapsed_time','time_diff','prior_question_had_explanation']].values)):\n",
    "                \n",
    "        \n",
    "        \n",
    "        if row[0] not in past_answer:\n",
    "            temp_answer = [0]*60\n",
    "            temp_past_answer = [0]*59 + [2]\n",
    "        else:\n",
    "            temp_answer = past_answer[row[0]].copy()\n",
    "            temp_past_answer = past_answer[row[0]].copy()\n",
    "        \n",
    "        \n",
    "        if row[0] not in past_question:\n",
    "            temp_question = [0]*59 + [13523*2+1]\n",
    "        else:\n",
    "            temp_question = past_question[row[0]].copy()\n",
    "        \n",
    "        \n",
    "        temp_past_answer= [x+1 if y > 0 else 0 for x , y in zip(temp_past_answer, temp_question)]\n",
    "        past_answer_correctly.append(temp_past_answer)\n",
    "        \n",
    "        temp_current_question = [x if x!= 13523*2+1 else 0 for x in temp_question]\n",
    "        temp_current_question.append(row[1]+1)\n",
    "        current_question.append(temp_current_question[1:])\n",
    "        \n",
    "        temp_past_question_answer = [x+y*13523 for x,y in zip(temp_question, temp_answer)]\n",
    "        past_question_answer.append(temp_past_question_answer)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if row[0] not in past_tag:\n",
    "            temp_tag = [0]*59 + [1520*2+1]\n",
    "        else:\n",
    "            temp_tag = past_tag[row[0]].copy()\n",
    "        \n",
    "        temp_current_tag = [x if x!= 1520*2+1 else 0 for x in temp_tag]\n",
    "        temp_current_tag.append(row[3]+1)\n",
    "        current_tag.append(temp_current_tag[1:])\n",
    "        \n",
    "        temp_past_tag_answer = [x+y*1520 for x,y in zip(temp_tag, temp_answer)]\n",
    "        past_tag_answer.append(temp_past_tag_answer)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        if row[0] not in past_part:\n",
    "            temp_part = [0]*59 + [15]\n",
    "        else:\n",
    "            temp_part = past_part[row[0]].copy()\n",
    "        \n",
    "        temp_current_part = [x if x!= 15 else 0 for x in temp_part]\n",
    "        temp_current_part.append(row[2])\n",
    "        current_part.append(temp_current_part[1:])\n",
    "        \n",
    "        temp_past_part_answer = [x+y*7 for x,y in zip(temp_part,temp_answer)]\n",
    "        past_part_answer.append(temp_past_part_answer)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if row[0] not in past_prior_elaps:\n",
    "            temp_elaps = [0]*59 + [row[4]/3e5]\n",
    "        else:\n",
    "            temp_elaps = past_prior_elaps[row[0]].copy()\n",
    "            temp_elaps.append(row[4]/3e5)\n",
    "        past_prior.append(temp_elaps)\n",
    "\n",
    "\n",
    "        if row[0] not in past_prior_exp:\n",
    "            temp_prior_exp = [0]*59 + [row[6]]\n",
    "        else:\n",
    "            temp_prior_exp = past_prior_exp[row[0]].copy()\n",
    "            temp_prior_exp.append(row[6])\n",
    "        past_prior_explanation.append(temp_prior_exp)\n",
    "\n",
    "        \n",
    "        if row[0] not in past_time_diff:\n",
    "            temp_time_diff = [0]*59 + [row[5]/1e6]\n",
    "        else:\n",
    "            temp_time_diff = past_time_diff[row[0]].copy()\n",
    "            temp_time_diff.append(row[5]/1e6)\n",
    "        past_time.append(temp_time_diff)\n",
    "    \n",
    "    current_part = np.array(current_part)\n",
    "    current_tag = np.array(current_tag)\n",
    "    current_question = np.array(current_question)\n",
    "    past_part_answer = np.array(past_part_answer)\n",
    "    past_tag_answer = np.array(past_tag_answer)\n",
    "    past_question_answer = np.array(past_question_answer)    \n",
    "    past_other_feats = np.dstack((past_prior,past_time))\n",
    "    past_answer_correctly = np.array(past_answer_correctly)\n",
    "    past_prior_explanation = np.array(past_prior_explanation)\n",
    "    \n",
    "    return current_part, current_tag, current_question, past_part_answer, past_tag_answer, past_question_answer, past_other_feats, past_answer_correctly, past_prior_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:38:06.357823Z",
     "iopub.status.busy": "2020-12-27T02:38:06.347533Z",
     "iopub.status.idle": "2020-12-27T02:38:06.383732Z",
     "shell.execute_reply": "2020-12-27T02:38:06.384391Z"
    },
    "papermill": {
     "duration": 0.093072,
     "end_time": "2020-12-27T02:38:06.384541",
     "exception": false,
     "start_time": "2020-12-27T02:38:06.291469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_user_feats_without_update(df , all_last60_avg, part1_last30_avg, part2_last30_avg, part3_last30_avg, \n",
    "                                  part4_last30_avg, part5_last30_avg, part6_last30_avg, part7_last30_avg, user_tag_lag1, user_answer_lag1,\n",
    "                                  last_time_u_dict, part1_answered_correctly_sum_u_dict, part1_count_u_dict, part2_answered_correctly_sum_u_dict,\n",
    "                                  part2_count_u_dict, part3_answered_correctly_sum_u_dict, part3_count_u_dict, part4_answered_correctly_sum_u_dict,\n",
    "                                  part4_count_u_dict, part5_answered_correctly_sum_u_dict, part5_count_u_dict, part6_answered_correctly_sum_u_dict,\n",
    "                                  part6_count_u_dict, part7_answered_correctly_sum_u_dict, part7_count_u_dict ):\n",
    "    acsu_part = np.zeros(len(df), dtype=np.int32)\n",
    "    cu_part = np.zeros(len(df), dtype=np.int32)\n",
    "    acsu = np.zeros(len(df), dtype=np.int32)\n",
    "    cu = np.zeros(len(df), dtype=np.int32)\n",
    "    td = np.zeros(len(df), dtype=np.int32)\n",
    "    user_prev_tag_lag1 = np.zeros(len(df), dtype = np.int32)\n",
    "    acsu_part_30 = np.zeros(len(df), dtype=np.float32)\n",
    "    acsu_60 = np.zeros(len(df), dtype=np.float32)\n",
    "    \n",
    "    for cnt,row in enumerate(df[['user_id','part','timestamp']].values):\n",
    "        td[cnt] = row[2] - last_time_u_dict[row[0]]\n",
    "        acsu[cnt] = part1_answered_correctly_sum_u_dict[row[0]]+part2_answered_correctly_sum_u_dict[row[0]]+part3_answered_correctly_sum_u_dict[row[0]]+part4_answered_correctly_sum_u_dict[row[0]]+part5_answered_correctly_sum_u_dict[row[0]]+part6_answered_correctly_sum_u_dict[row[0]]+part7_answered_correctly_sum_u_dict[row[0]]\n",
    "        cu[cnt] = part1_count_u_dict[row[0]]+part2_count_u_dict[row[0]]+part3_count_u_dict[row[0]]+part4_count_u_dict[row[0]]+part5_count_u_dict[row[0]]+part6_count_u_dict[row[0]]+part7_count_u_dict[row[0]]\n",
    "\n",
    "            \n",
    "            \n",
    "        if row[0] not in all_last60_avg:\n",
    "            acsu_60[cnt] = 0\n",
    "        else:\n",
    "            acsu_60[cnt] = sum(all_last60_avg[row[0]])/len(all_last60_avg[row[0]])\n",
    "            \n",
    "        \n",
    "        if row[0] in user_tag_lag1:\n",
    "            user_prev_tag_lag1[cnt] = user_tag_lag1[row[0]]+user_answer_lag1[row[0]]*(283+1)\n",
    "        else:\n",
    "            user_prev_tag_lag1[cnt] = 568\n",
    "            \n",
    "            \n",
    "        if row[1] == 1:\n",
    "            acsu_part[cnt] = part1_answered_correctly_sum_u_dict[row[0]]\n",
    "            cu_part[cnt] = part1_count_u_dict[row[0]]\n",
    "            \n",
    "            if row[0] not in part1_last30_avg:\n",
    "                acsu_part_30[cnt] = 0\n",
    "            else:\n",
    "                acsu_part_30[cnt] = sum(part1_last30_avg[row[0]])/len(part1_last30_avg[row[0]])  \n",
    "            \n",
    "            \n",
    "        elif row[1] == 2:\n",
    "            acsu_part[cnt] = part2_answered_correctly_sum_u_dict[row[0]]\n",
    "            cu_part[cnt] = part2_count_u_dict[row[0]]\n",
    "\n",
    "            if row[0] not in part2_last30_avg:\n",
    "                acsu_part_30[cnt] = 0\n",
    "            else:\n",
    "                acsu_part_30[cnt] = sum(part2_last30_avg[row[0]])/len(part2_last30_avg[row[0]])  \n",
    "            \n",
    "            \n",
    "        elif row[1] == 3:\n",
    "            acsu_part[cnt] = part3_answered_correctly_sum_u_dict[row[0]]\n",
    "            cu_part[cnt] = part3_count_u_dict[row[0]]\n",
    "\n",
    "            if row[0] not in part3_last30_avg:\n",
    "                acsu_part_30[cnt] = 0\n",
    "            else:\n",
    "                acsu_part_30[cnt] = sum(part3_last30_avg[row[0]])/len(part3_last30_avg[row[0]])  \n",
    "            \n",
    "            \n",
    "        elif row[1] == 4:\n",
    "            acsu_part[cnt] = part4_answered_correctly_sum_u_dict[row[0]]\n",
    "            cu_part[cnt] = part4_count_u_dict[row[0]]\n",
    "            if row[0] not in part4_last30_avg:\n",
    "                acsu_part_30[cnt] = 0\n",
    "            else:\n",
    "                acsu_part_30[cnt] = sum(part4_last30_avg[row[0]])/len(part4_last30_avg[row[0]])      \n",
    "                \n",
    "                \n",
    "        elif row[1] == 5:\n",
    "            acsu_part[cnt] = part5_answered_correctly_sum_u_dict[row[0]]\n",
    "            cu_part[cnt] = part5_count_u_dict[row[0]]\n",
    "            if row[0] not in part5_last30_avg:\n",
    "                acsu_part_30[cnt] = 0\n",
    "            else:\n",
    "                acsu_part_30[cnt] = sum(part5_last30_avg[row[0]])/len(part5_last30_avg[row[0]])         \n",
    "                \n",
    "                \n",
    "        elif row[1] == 6:\n",
    "            acsu_part[cnt] = part6_answered_correctly_sum_u_dict[row[0]]\n",
    "            cu_part[cnt] = part6_count_u_dict[row[0]]\n",
    "            if row[0] not in part6_last30_avg:\n",
    "                acsu_part_30[cnt] = 0\n",
    "            else:\n",
    "                acsu_part_30[cnt] = sum(part6_last30_avg[row[0]])/len(part6_last30_avg[row[0]])  \n",
    "                \n",
    "                \n",
    "        elif row[1] == 7:\n",
    "            acsu_part[cnt] = part7_answered_correctly_sum_u_dict[row[0]]\n",
    "            cu_part[cnt] = part7_count_u_dict[row[0]]\n",
    "            if row[0] not in part7_last30_avg:\n",
    "                acsu_part_30[cnt] = 0\n",
    "            else:\n",
    "                acsu_part_30[cnt] = sum(part7_last30_avg[row[0]])/len(part7_last30_avg[row[0]])  \n",
    "                \n",
    "                \n",
    "    user_feats_df = pd.DataFrame({'answered_correctly_sum_u':acsu, 'count_u':cu, 'part_answered_correctly_sum_u':acsu_part, 'part_count_u':cu_part, 'time_diff':td,'user_prev_tag_lag1' : user_prev_tag_lag1, 'last_60':acsu_60, 'part_last_30': acsu_part_30})#, 'user_prev_tag_lag2': user_prev_tag_lag2, 'user_prev_tag_lag3': user_prev_tag_lag3, 'user_prev_tag_lag4': user_prev_tag_lag4, 'user_prev_tag_lag5': user_prev_tag_lag5})\n",
    "    user_feats_df['answered_correctly_avg_u'] = user_feats_df['answered_correctly_sum_u'] / user_feats_df['count_u']\n",
    "    user_feats_df['part_answered_correctly_avg_u'] = user_feats_df['part_answered_correctly_sum_u'] / user_feats_df['part_count_u']\n",
    "    df = pd.concat([df, user_feats_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:38:06.515623Z",
     "iopub.status.busy": "2020-12-27T02:38:06.479489Z",
     "iopub.status.idle": "2020-12-27T02:38:06.546666Z",
     "shell.execute_reply": "2020-12-27T02:38:06.545951Z"
    },
    "papermill": {
     "duration": 0.123386,
     "end_time": "2020-12-27T02:38:06.546802",
     "exception": false,
     "start_time": "2020-12-27T02:38:06.423416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_user_feats(df , all_last60_avg, part1_last30_avg, part2_last30_avg, part3_last30_avg\n",
    "                      , part4_last30_avg, part5_last30_avg, part6_last30_avg, part7_last30_avg , user_tag_lag1, user_answer_lag1 ,\n",
    "                      last_time_u_dict, part1_answered_correctly_sum_u_dict, part1_count_u_dict, \n",
    "                      part2_answered_correctly_sum_u_dict, part2_count_u_dict, part3_answered_correctly_sum_u_dict, part3_count_u_dict,\n",
    "                      part4_answered_correctly_sum_u_dict, part4_count_u_dict, part5_answered_correctly_sum_u_dict, part5_count_u_dict,\n",
    "                      part6_answered_correctly_sum_u_dict, part6_count_u_dict, part7_answered_correctly_sum_u_dict, part7_count_u_dict, \n",
    "                      past_question, past_tag, past_part, past_answer, past_prior_elaps, past_time_diff, past_prior_exp):\n",
    "    for row in df[['user_id','answered_correctly','content_type_id','part','timestamp','tag_sum','content_id','tag_num','prior_question_elapsed_time','time_diff', 'prior_question_had_explanation']].values:\n",
    "        if row[2] == 0:                \n",
    "            if row[0] not in past_question:\n",
    "                new_list = deque([0]*60, maxlen = 60)\n",
    "                new_list.append(13523*2+1)\n",
    "                new_list.append(row[6]+1)\n",
    "                past_question[row[0]] = new_list\n",
    "            else:\n",
    "                past_question[row[0]].append(row[6]+1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            if row[0] not in past_tag:\n",
    "                new_list = deque([0]*60, maxlen = 60)\n",
    "                new_list.append(1520*2+1)\n",
    "                new_list.append(row[7]+1)\n",
    "                past_tag[row[0]] = new_list\n",
    "            else:\n",
    "                past_tag[row[0]].append(row[7]+1)            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            if row[0] not in past_part:\n",
    "                new_list = deque([0]*60, maxlen = 60)\n",
    "                new_list.append(15)\n",
    "                new_list.append(row[3])\n",
    "                past_part[row[0]] = new_list\n",
    "            else:\n",
    "                past_part[row[0]].append(row[3])     \n",
    "\n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            if row[0] not in past_answer:\n",
    "                new_list = deque([0]*60, maxlen = 60)\n",
    "                new_list.append(2)\n",
    "                new_list.append(row[1])\n",
    "                past_answer[row[0]] = new_list\n",
    "            else:\n",
    "                past_answer[row[0]].append(row[1])     \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            if row[0] not in past_prior_elaps:\n",
    "                new_list = deque([0]*60, maxlen = 60)\n",
    "                new_list.append(row[8]/3e5)\n",
    "                past_prior_elaps[row[0]] = new_list\n",
    "            else:\n",
    "                past_prior_elaps[row[0]].append(row[8]/3e5)   \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            if row[0] not in past_time_diff:\n",
    "                new_list = deque([0]*60, maxlen = 60)\n",
    "                if row[9] >= 1e6:\n",
    "                    new_list.append(1.0)\n",
    "                else:\n",
    "                    new_list.append(row[9]/1e6)\n",
    "                past_time_diff[row[0]] = new_list\n",
    "            else:\n",
    "                if row[9] >= 1e6:\n",
    "                    past_time_diff[row[0]].append(1.0)   \n",
    "                else:\n",
    "                    past_time_diff[row[0]].append(row[9]/1e6)   \n",
    "             \n",
    "            \n",
    "            \n",
    "                    \n",
    "            if row[0] not in past_prior_exp:\n",
    "                new_list = deque([0]*60, maxlen = 60)\n",
    "                new_list.append(row[10])\n",
    "                past_prior_exp[row[0]] = new_list\n",
    "            else:\n",
    "                past_prior_exp[row[0]].append(row[10])                    \n",
    "            \n",
    "            \n",
    "            \n",
    "                    \n",
    "            if row[0] not in all_last60_avg:\n",
    "                new_list = deque(maxlen = 60)\n",
    "                new_list.append(row[1])\n",
    "                all_last60_avg[row[0]] = new_list\n",
    "            else:\n",
    "                all_last60_avg[row[0]].append(row[1])            \n",
    "\n",
    "                \n",
    "                \n",
    "            user_tag_lag1[row[0]] = row[5]\n",
    "            \n",
    "            user_answer_lag1[row[0]] = row[1]   \n",
    "            \n",
    "            last_time_u_dict[row[0]] = row[4]\n",
    "            \n",
    "            if row[3] == 1:\n",
    "                part1_answered_correctly_sum_u_dict[row[0]] += row[1]\n",
    "                part1_count_u_dict[row[0]] += 1\n",
    "                if row[0] not in part1_last30_avg:\n",
    "                    new_list = deque(maxlen = 30)\n",
    "                    new_list.append(row[1])\n",
    "                    part1_last30_avg[row[0]] = new_list\n",
    "                else:\n",
    "                    part1_last30_avg[row[0]].append(row[1])\n",
    "                \n",
    "            \n",
    "            elif row[3] == 2:\n",
    "                part2_answered_correctly_sum_u_dict[row[0]] += row[1]\n",
    "                part2_count_u_dict[row[0]] += 1   \n",
    "                if row[0] not in part2_last30_avg:\n",
    "                    new_list = deque(maxlen = 30)\n",
    "                    new_list.append(row[1])\n",
    "                    part2_last30_avg[row[0]] = new_list\n",
    "                else:\n",
    "                    part2_last30_avg[row[0]].append(row[1])\n",
    "                \n",
    "            \n",
    "            elif row[3] == 3:\n",
    "                part3_answered_correctly_sum_u_dict[row[0]] += row[1]\n",
    "                part3_count_u_dict[row[0]] += 1   \n",
    "                if row[0] not in part3_last30_avg:\n",
    "                    new_list = deque(maxlen = 30)\n",
    "                    new_list.append(row[1])\n",
    "                    part3_last30_avg[row[0]] = new_list\n",
    "                else:\n",
    "                    part3_last30_avg[row[0]].append(row[1])\n",
    "                \n",
    "            \n",
    "            elif row[3] == 4:\n",
    "                part4_answered_correctly_sum_u_dict[row[0]] += row[1]\n",
    "                part4_count_u_dict[row[0]] += 1     \n",
    "                if row[0] not in part4_last30_avg:\n",
    "                    new_list = deque(maxlen = 30)\n",
    "                    new_list.append(row[1])\n",
    "                    part4_last30_avg[row[0]] = new_list\n",
    "                else:\n",
    "                    part4_last30_avg[row[0]].append(row[1])\n",
    "                    \n",
    "            \n",
    "            elif row[3] == 5:\n",
    "                part5_answered_correctly_sum_u_dict[row[0]] += row[1]\n",
    "                part5_count_u_dict[row[0]] += 1   \n",
    "                if row[0] not in part5_last30_avg:\n",
    "                    new_list = deque(maxlen = 30)\n",
    "                    new_list.append(row[1])\n",
    "                    part5_last30_avg[row[0]] = new_list\n",
    "                else:\n",
    "                    part5_last30_avg[row[0]].append(row[1])\n",
    "                    \n",
    "            \n",
    "            elif row[3] == 6:\n",
    "                part6_answered_correctly_sum_u_dict[row[0]] += row[1]\n",
    "                part6_count_u_dict[row[0]] += 1 \n",
    "                if row[0] not in part6_last30_avg:\n",
    "                    new_list = deque(maxlen = 30)\n",
    "                    new_list.append(row[1])\n",
    "                    part6_last30_avg[row[0]] = new_list\n",
    "                else:\n",
    "                    part6_last30_avg[row[0]].append(row[1])\n",
    "                \n",
    "                \n",
    "            elif row[3] == 7:\n",
    "                part7_answered_correctly_sum_u_dict[row[0]] += row[1]\n",
    "                part7_count_u_dict[row[0]] += 1\n",
    "                if row[0] not in part7_last30_avg:\n",
    "                    new_list = deque(maxlen = 30)\n",
    "                    new_list.append(row[1])\n",
    "                    part7_last30_avg[row[0]] = new_list\n",
    "                else:\n",
    "                    part7_last30_avg[row[0]].append(row[1])\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:38:06.644454Z",
     "iopub.status.busy": "2020-12-27T02:38:06.643096Z",
     "iopub.status.idle": "2020-12-27T02:39:04.738609Z",
     "shell.execute_reply": "2020-12-27T02:39:04.737391Z"
    },
    "papermill": {
     "duration": 58.152487,
     "end_time": "2020-12-27T02:39:04.738733",
     "exception": false,
     "start_time": "2020-12-27T02:38:06.586246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "# read data\n",
    "feld_needed = ['user_id','content_id','answered_correctly','prior_question_elapsed_time','prior_question_had_explanation']\n",
    "train = pd.read_pickle(train_pickle)[feld_needed]\n",
    "if debug:\n",
    "    train = train[:1000000]\n",
    "    valid = valid[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:39:04.804905Z",
     "iopub.status.busy": "2020-12-27T02:39:04.804029Z",
     "iopub.status.idle": "2020-12-27T02:39:10.346605Z",
     "shell.execute_reply": "2020-12-27T02:39:10.345210Z"
    },
    "papermill": {
     "duration": 5.584304,
     "end_time": "2020-12-27T02:39:10.346719",
     "exception": false,
     "start_time": "2020-12-27T02:39:04.762415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.loc[train.answered_correctly != -1].reset_index(drop=True)\n",
    "_=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:39:10.396492Z",
     "iopub.status.busy": "2020-12-27T02:39:10.395557Z",
     "iopub.status.idle": "2020-12-27T02:39:11.359958Z",
     "shell.execute_reply": "2020-12-27T02:39:11.361081Z"
    },
    "papermill": {
     "duration": 0.992083,
     "end_time": "2020-12-27T02:39:11.361254",
     "exception": false,
     "start_time": "2020-12-27T02:39:10.369171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prior_question_elapsed_time_mean = train.prior_question_elapsed_time.dropna().values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:39:11.452099Z",
     "iopub.status.busy": "2020-12-27T02:39:11.451327Z",
     "iopub.status.idle": "2020-12-27T02:39:16.045115Z",
     "shell.execute_reply": "2020-12-27T02:39:16.044134Z"
    },
    "papermill": {
     "duration": 4.644881,
     "end_time": "2020-12-27T02:39:16.045230",
     "exception": false,
     "start_time": "2020-12-27T02:39:11.400349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "content_df = train[['content_id','answered_correctly']].groupby(['content_id']).agg(['mean','std']).reset_index()\n",
    "content_df.columns = ['content_id', 'answered_correctly_avg_c','answered_correctly_std_c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:39:16.103397Z",
     "iopub.status.busy": "2020-12-27T02:39:16.102863Z",
     "iopub.status.idle": "2020-12-27T02:39:35.110140Z",
     "shell.execute_reply": "2020-12-27T02:39:35.110917Z"
    },
    "papermill": {
     "duration": 19.042302,
     "end_time": "2020-12-27T02:39:35.111070",
     "exception": false,
     "start_time": "2020-12-27T02:39:16.068768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "train_time_diff = pd.read_csv('../input/for-lgbm/train_time_diff.csv')\n",
    "train = pd.concat([train,train_time_diff], axis = 1)\n",
    "train.time_diff.loc[train.time_diff >= 1e6] = 1e6\n",
    "del(train_time_diff)\n",
    "_=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:39:36.151564Z",
     "iopub.status.busy": "2020-12-27T02:39:36.150823Z",
     "iopub.status.idle": "2020-12-27T02:39:46.721810Z",
     "shell.execute_reply": "2020-12-27T02:39:46.721294Z"
    },
    "papermill": {
     "duration": 11.587085,
     "end_time": "2020-12-27T02:39:46.721917",
     "exception": false,
     "start_time": "2020-12-27T02:39:35.134832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "content_df2 = train[['content_id','time_diff']].groupby(['content_id']).agg(['median']).reset_index()\n",
    "content_df2.columns = ['content_id', 'time_diff_average_c']\n",
    "\n",
    "content_df = content_df.merge(content_df2, on = 'content_id', how = 'left')\n",
    "\n",
    "#train['prior_question_had_explanation'] = train['prior_question_had_explanation']*1\n",
    "content_df2 = train[['content_id','prior_question_had_explanation']].groupby(['content_id']).agg(['mean']).reset_index()\n",
    "content_df2.columns = ['content_id','prior_has_explanation_average_c']\n",
    "content_df = content_df.merge(content_df2, on = 'content_id', how = 'left')\n",
    "\n",
    "del(content_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:39:46.852967Z",
     "iopub.status.busy": "2020-12-27T02:39:46.851301Z",
     "iopub.status.idle": "2020-12-27T02:39:46.853551Z",
     "shell.execute_reply": "2020-12-27T02:39:46.853948Z"
    },
    "papermill": {
     "duration": 0.104088,
     "end_time": "2020-12-27T02:39:46.854062",
     "exception": false,
     "start_time": "2020-12-27T02:39:46.749974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del(train)\n",
    "_=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:39:46.911617Z",
     "iopub.status.busy": "2020-12-27T02:39:46.911044Z",
     "iopub.status.idle": "2020-12-27T02:39:46.949594Z",
     "shell.execute_reply": "2020-12-27T02:39:46.949110Z"
    },
    "papermill": {
     "duration": 0.0714,
     "end_time": "2020-12-27T02:39:46.949687",
     "exception": false,
     "start_time": "2020-12-27T02:39:46.878287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions_df = pd.read_csv(question_file)\n",
    "questions_df.tags.fillna('-1-1', inplace = True)\n",
    "questions_df['tag_sum'] = pd.factorize(questions_df.tag_sum)[0]\n",
    "questions_df['tag_num'] = pd.factorize(questions_df.tags)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023784,
     "end_time": "2020-12-27T02:39:46.998258",
     "exception": false,
     "start_time": "2020-12-27T02:39:46.974474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TRANSFORMET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:39:47.057053Z",
     "iopub.status.busy": "2020-12-27T02:39:47.056272Z",
     "iopub.status.idle": "2020-12-27T02:39:52.209289Z",
     "shell.execute_reply": "2020-12-27T02:39:52.208670Z"
    },
    "papermill": {
     "duration": 5.187404,
     "end_time": "2020-12-27T02:39:52.209419",
     "exception": false,
     "start_time": "2020-12-27T02:39:47.022015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, LSTM, Masking, Embedding, Concatenate, Input, Reshape,Flatten, AveragePooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Lambda\n",
    "#from tensorflow.keras.layers import merge\n",
    "from tensorflow.keras.layers import multiply, Reshape\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:39:52.321763Z",
     "iopub.status.busy": "2020-12-27T02:39:52.285605Z",
     "iopub.status.idle": "2020-12-27T02:39:52.332193Z",
     "shell.execute_reply": "2020-12-27T02:39:52.332811Z"
    },
    "papermill": {
     "duration": 0.097638,
     "end_time": "2020-12-27T02:39:52.332927",
     "exception": false,
     "start_time": "2020-12-27T02:39:52.235289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "      q: query shape == (..., seq_len_q, depth)\n",
    "      k: key shape == (..., seq_len_k, depth)\n",
    "      v: value shape == (..., seq_len_v, depth_v)\n",
    "      mask: Float tensor with shape broadcastable \n",
    "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "      output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                       (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, e, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, e, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(e + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                   maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, e, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        #x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, e, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#class Transformer(tf.keras.Model):\n",
    "#    def __init__(self, num_layers, en_d_model, en_num_heads, dff, pe_input, rate=0.2):\n",
    "#        super(Transformer, self).__init__()\n",
    "\n",
    "#        self.encoder = Encoder(num_layers, en_d_model, en_num_heads, dff, \n",
    "#                           pe_input, rate)\n",
    "\n",
    "\n",
    "#        self.second_final_layer = tf.keras.layers.Dense(dff)\n",
    "#        self.final_layer = Dense(1,activation = 'sigmoid')\n",
    "    \n",
    "#    def call(self, inp1, inp2, training, mask):\n",
    "\n",
    "#        enc_output = self.encoder(inp1, inp2, training, mask)  # (batch_size, inp_seq_len, d_model)\n",
    "            \n",
    "#        second_final_output = self.second_final_layer(enc_output)  # (batch_size, tar_seq_len, question_answer_pair_size)\n",
    "#        final_output = self.final_layer(second_final_output)\n",
    "#        return final_output\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, enc_output, training, \n",
    "                look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "                enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "    \n",
    "    \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                    maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, \n",
    "               look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        #x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "              x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "        attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "        attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights    \n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, padding_length, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, padding_length)\n",
    "\n",
    "        self.second_final_layer = tf.keras.layers.Dense(dff)\n",
    "        self.final_layer = Dense(1,activation = 'sigmoid')\n",
    "    \n",
    "    def call(self, inp1, inp2, training, de_look_ahead_mask, de_padding_mask):\n",
    "\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "                inp2, inp1, training, de_look_ahead_mask, de_padding_mask)\n",
    "            \n",
    "        second_final_output = self.second_final_layer(dec_output)  # (batch_size, tar_seq_len, question_answer_pair_size)\n",
    "        final_output = self.final_layer(second_final_output)\n",
    "        return final_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:39:52.403247Z",
     "iopub.status.busy": "2020-12-27T02:39:52.402734Z",
     "iopub.status.idle": "2020-12-27T02:39:57.798617Z",
     "shell.execute_reply": "2020-12-27T02:39:57.798065Z"
    },
    "papermill": {
     "duration": 5.440088,
     "end_time": "2020-12-27T02:39:57.798728",
     "exception": false,
     "start_time": "2020-12-27T02:39:52.358640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_layers = 6\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 512\n",
    "\n",
    "n_question = 13524\n",
    "n_tag = 1521\n",
    "n_part = 8\n",
    "n_answer = 4\n",
    "n_prev_q_exp = 4\n",
    "\n",
    "pe_input = 60\n",
    "\n",
    "\n",
    "\n",
    "def build(num_layers, d_model, num_heads, dff, n_answer, n_question, n_tag, n_part, n_prev_q_exp, pe_input):\n",
    "    masking_func = lambda inputs, previous_mask: previous_mask\n",
    "    \n",
    "    en_input1 = Input(batch_shape = (None, None), name = 'past_answer')\n",
    "    en_input1_embed = Embedding(n_answer, d_model)(en_input1)\n",
    "    en_input2 = Input(batch_shape = (None, None, 1), name = 'other_feature1')\n",
    "    en_input2_masked = (Masking(mask_value= 0, input_shape = (None, None, 1)))(en_input2)\n",
    "    en_input2_embed = Dense(d_model, input_shape = (None, None, 1))(en_input2_masked)\n",
    "    en_input3 = Input(batch_shape = (None, None, 1), name = 'other_feature2')\n",
    "    en_input3_masked = (Masking(mask_value= 0, input_shape = (None, None, 1)))(en_input2)\n",
    "    en_input3_embed = Dense(d_model, input_shape = (None, None, 1))(en_input2_masked)\n",
    "    en_input4 = Input(batch_shape = (None, None), name = 'past_prior_exp')\n",
    "    en_input4_embed = Embedding(n_prev_q_exp, d_model)(en_input4)\n",
    "    \n",
    "    \n",
    "    en_input_embed_sum = tf.math.add_n([en_input1_embed])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #en_input1_embed = K.sum(en_input1_embed, axis = -2)\n",
    "    en_input5 = Input(batch_shape = (None, None), name = 'current_question')\n",
    "    en_input5_embed = Embedding(n_question, d_model)(en_input5)\n",
    "    en_input6 = Input(batch_shape = (None, None), name = 'current_tag')\n",
    "    en_input6_embed = Embedding(n_tag, d_model)(en_input6)\n",
    "    en_input7 = Input(batch_shape = (None, None), name = 'current_part')\n",
    "    en_input7_embed = Embedding(n_part, d_model)(en_input7)\n",
    "    en_input_embed_sum2 = tf.math.add_n([en_input5_embed, en_input6_embed, en_input7_embed, en_input2_embed, en_input3_embed, en_input4_embed])\n",
    "    \n",
    "    \n",
    "    #en_input2_embed = K.sum(en_input2_embed, axis = -2)\n",
    "\n",
    "    \n",
    "    \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(en_input_embed_sum)[1])\n",
    "    padding_mask = create_padding_mask(en_input1)\n",
    "    combined_mask = tf.maximum(look_ahead_mask, padding_mask)\n",
    "    \n",
    "    \n",
    "    transformer = Transformer(num_layers, d_model, num_heads, dff, pe_input)\n",
    "    \n",
    "    final_output = transformer(en_input_embed_sum, en_input_embed_sum2, False, combined_mask, combined_mask)\n",
    "\n",
    "    \n",
    "    model = Model(inputs=[en_input1, en_input2, en_input3, en_input4, en_input5, en_input6, en_input7], outputs=final_output)\n",
    "    model.compile( optimizer = 'adam',\n",
    "                    loss = 'binary_crossentropy',\n",
    "                    metrics=['accuracy',AUC()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "sakt_model = build(num_layers, d_model, num_heads, dff, n_answer, n_question, n_tag, n_part, n_prev_q_exp, pe_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:39:57.866726Z",
     "iopub.status.busy": "2020-12-27T02:39:57.866156Z",
     "iopub.status.idle": "2020-12-27T02:39:58.328417Z",
     "shell.execute_reply": "2020-12-27T02:39:58.327908Z"
    },
    "papermill": {
     "duration": 0.505145,
     "end_time": "2020-12-27T02:39:58.328535",
     "exception": false,
     "start_time": "2020-12-27T02:39:57.823390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sakt_model.load_weights('../input/my-sakt/Transformer_model_feature_extraction.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023839,
     "end_time": "2020-12-27T02:39:58.377266",
     "exception": false,
     "start_time": "2020-12-27T02:39:58.353427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SAINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:39:58.465262Z",
     "iopub.status.busy": "2020-12-27T02:39:58.439297Z",
     "iopub.status.idle": "2020-12-27T02:39:58.467930Z",
     "shell.execute_reply": "2020-12-27T02:39:58.467494Z"
    },
    "papermill": {
     "duration": 0.066918,
     "end_time": "2020-12-27T02:39:58.468012",
     "exception": false,
     "start_time": "2020-12-27T02:39:58.401094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayer2(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer2, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "\n",
    "class Encoder2(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                   maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder2, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer2(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        #x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    \n",
    "class DecoderLayer2(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer2, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, enc_output, training, \n",
    "                look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "                enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "    \n",
    "    \n",
    "class Decoder2(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                    maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder2, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer2(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, \n",
    "               look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        #x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "              x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "        attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "        attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights    \n",
    "\n",
    "class Transformer2(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, padding_length, rate=0.1):\n",
    "        super(Transformer2, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder2(num_layers, d_model, num_heads, dff, padding_length)\n",
    "        \n",
    "        self.decoder = Decoder2(num_layers, d_model, num_heads, dff, padding_length)\n",
    "\n",
    "        self.second_final_layer = tf.keras.layers.Dense(dff)\n",
    "        self.final_layer = Dense(1,activation = 'sigmoid')\n",
    "    \n",
    "    def call(self, inp1, inp2, training, en_combined_mask, de_look_ahead_mask, de_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp1, training, en_combined_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "                inp2, enc_output, training, de_look_ahead_mask, de_padding_mask)\n",
    "            \n",
    "        second_final_output = self.second_final_layer(dec_output)  # (batch_size, tar_seq_len, question_answer_pair_size)\n",
    "        final_output = self.final_layer(second_final_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:39:58.541051Z",
     "iopub.status.busy": "2020-12-27T02:39:58.540201Z",
     "iopub.status.idle": "2020-12-27T02:40:01.962635Z",
     "shell.execute_reply": "2020-12-27T02:40:01.961697Z"
    },
    "papermill": {
     "duration": 3.471018,
     "end_time": "2020-12-27T02:40:01.962747",
     "exception": false,
     "start_time": "2020-12-27T02:39:58.491729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 160\n",
    "num_heads = 8\n",
    "dff = 160*4\n",
    "\n",
    "n_question = 13524\n",
    "n_tag = 1521\n",
    "n_part = 8\n",
    "n_answer = 4\n",
    "n_prev_q_exp = 4\n",
    "\n",
    "pe_input = 60\n",
    "\n",
    "\n",
    "def build(num_layers, d_model, num_heads, dff, n_question, n_tag, n_part, n_answer, n_prev_q_exp, pe_input):\n",
    "\n",
    "    en_input1 = Input(batch_shape = (None, None), name = 'current_question')\n",
    "    en_input1_embed = Embedding(n_question, d_model)(en_input1)\n",
    "    en_input2 = Input(batch_shape = (None, None), name = 'current_tag')\n",
    "    en_input2_embed = Embedding(n_tag, d_model)(en_input2)\n",
    "    en_input3 = Input(batch_shape = (None, None), name = 'current_part')\n",
    "    en_input3_embed = Embedding(n_part, d_model)(en_input3)\n",
    "\n",
    "    en_look_ahead_mask = create_look_ahead_mask(tf.shape(en_input1)[1])\n",
    "    en_padding_mask = create_padding_mask(en_input1)\n",
    "    en_combined_mask = tf.maximum(en_look_ahead_mask, en_padding_mask)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #en_input1_embed = K.sum(en_input1_embed, axis = -2)\n",
    "    de_input4 = Input(batch_shape = (None, None), name = 'past_answer')\n",
    "    de_input4_embed = Embedding(n_answer, d_model)(de_input4)\n",
    "    de_input5 = Input(batch_shape = (None, None, 1), name = 'other_feature1')\n",
    "    de_input5_masked = (Masking(mask_value= 0, input_shape = (None, None, 1)))(de_input5)\n",
    "    de_input5_embed = Dense(d_model, input_shape = (None, None, 1))(de_input5_masked)\n",
    "    de_input6 = Input(batch_shape = (None, None, 1), name = 'other_feature2')\n",
    "    de_input6_masked = (Masking(mask_value= 0, input_shape = (None, None, 1)))(de_input6)\n",
    "    de_input6_embed = Dense(d_model, input_shape = (None, None, 1))(de_input6_masked)   \n",
    "    de_input7 = Input(batch_shape = (None,None), name = 'prev_q_exp')\n",
    "    de_input7_embed = Embedding(n_prev_q_exp, d_model)(de_input7)\n",
    "    de_input = tf.math.add_n([de_input4_embed, de_input5_embed, de_input6_embed, de_input7_embed])\n",
    "\n",
    "    en_input = tf.math.add_n([en_input1_embed, en_input2_embed, en_input3_embed])\n",
    "\n",
    "    \n",
    "    \n",
    "    de_look_ahead_mask = create_look_ahead_mask(tf.shape(de_input4)[1])\n",
    "    de_padding_mask = create_padding_mask(de_input4)\n",
    "    de_combined_mask = tf.maximum(de_look_ahead_mask, de_padding_mask)\n",
    "    \n",
    "    \n",
    "    transformer = Transformer2(num_layers, d_model, num_heads, dff, pe_input)\n",
    "    \n",
    "    final_output = transformer(en_input, de_input, False, en_combined_mask, de_combined_mask, de_combined_mask)\n",
    "    \n",
    "    #with tpu_strategy.scope():\n",
    "    model = Model(inputs=[en_input1, en_input2, en_input3, de_input4, de_input5, de_input6, de_input7], outputs=final_output)\n",
    "    #    model.build()\n",
    "    #    model.compile( optimizer = 'adam',\n",
    "    #                    loss = 'binary_crossentropy',\n",
    "    #                    metrics=['accuracy',AUC()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "saint_model1 = build(num_layers, d_model, num_heads, dff, n_question, n_tag, n_part, n_answer, n_prev_q_exp, pe_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:40:02.381243Z",
     "iopub.status.busy": "2020-12-27T02:40:02.049961Z",
     "iopub.status.idle": "2020-12-27T02:40:02.776234Z",
     "shell.execute_reply": "2020-12-27T02:40:02.775059Z"
    },
    "papermill": {
     "duration": 0.78923,
     "end_time": "2020-12-27T02:40:02.776376",
     "exception": false,
     "start_time": "2020-12-27T02:40:01.987146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "saint_model1.load_weights('../input/saint-model4/SAINT_model_feature_extraction.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024275,
     "end_time": "2020-12-27T02:40:02.825524",
     "exception": false,
     "start_time": "2020-12-27T02:40:02.801249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SAINT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:40:02.906845Z",
     "iopub.status.busy": "2020-12-27T02:40:02.894956Z",
     "iopub.status.idle": "2020-12-27T02:40:05.796348Z",
     "shell.execute_reply": "2020-12-27T02:40:05.795815Z"
    },
    "papermill": {
     "duration": 2.945495,
     "end_time": "2020-12-27T02:40:05.796469",
     "exception": false,
     "start_time": "2020-12-27T02:40:02.850974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 512\n",
    "\n",
    "n_question = 13524\n",
    "n_tag = 1521\n",
    "n_part = 8\n",
    "n_answer = 4\n",
    "n_prev_q_exp = 4\n",
    "\n",
    "pe_input = 60\n",
    "\n",
    "\n",
    "def build(num_layers, d_model, num_heads, dff, n_question, n_tag, n_part, n_answer, n_prev_q_exp, pe_input):\n",
    "\n",
    "    en_input1 = Input(batch_shape = (None, None), name = 'current_question')\n",
    "    en_input1_embed = Embedding(n_question, d_model)(en_input1)\n",
    "    en_input2 = Input(batch_shape = (None, None), name = 'current_tag')\n",
    "    en_input2_embed = Embedding(n_tag, d_model)(en_input2)\n",
    "    en_input3 = Input(batch_shape = (None, None), name = 'current_part')\n",
    "    en_input3_embed = Embedding(n_part, d_model)(en_input3)\n",
    "\n",
    "    en_look_ahead_mask = create_look_ahead_mask(tf.shape(en_input1)[1])\n",
    "    en_padding_mask = create_padding_mask(en_input1)\n",
    "    en_combined_mask = tf.maximum(en_look_ahead_mask, en_padding_mask)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #en_input1_embed = K.sum(en_input1_embed, axis = -2)\n",
    "    de_input4 = Input(batch_shape = (None, None), name = 'past_answer')\n",
    "    de_input4_embed = Embedding(n_answer, d_model)(de_input4)\n",
    "    de_input5 = Input(batch_shape = (None, None, 1), name = 'other_feature1')\n",
    "    de_input5_masked = (Masking(mask_value= 0, input_shape = (None, None, 1)))(de_input5)\n",
    "    de_input5_embed = Dense(d_model, input_shape = (None, None, 1))(de_input5_masked)\n",
    "    de_input6 = Input(batch_shape = (None, None, 1), name = 'other_feature2')\n",
    "    de_input6_masked = (Masking(mask_value= 0, input_shape = (None, None, 1)))(de_input6)\n",
    "    de_input6_embed = Dense(d_model, input_shape = (None, None, 1))(de_input6_masked)   \n",
    "    de_input7 = Input(batch_shape = (None,None), name = 'prev_q_exp')\n",
    "    de_input7_embed = Embedding(n_prev_q_exp, d_model)(de_input7)\n",
    "    de_input = tf.math.add_n([de_input4_embed])#, de_input5_embed, de_input6_embed, de_input7_embed])\n",
    "\n",
    "    en_input = tf.math.add_n([en_input1_embed, en_input2_embed, en_input3_embed, de_input5_embed, de_input6_embed, de_input7_embed])\n",
    "\n",
    "    \n",
    "    \n",
    "    de_look_ahead_mask = create_look_ahead_mask(tf.shape(de_input4)[1])\n",
    "    de_padding_mask = create_padding_mask(de_input4)\n",
    "    de_combined_mask = tf.maximum(de_look_ahead_mask, de_padding_mask)\n",
    "    \n",
    "    \n",
    "    transformer = Transformer2(num_layers, d_model, num_heads, dff, pe_input)\n",
    "    \n",
    "    final_output = transformer(en_input, de_input, False, en_combined_mask, de_combined_mask, de_combined_mask)\n",
    "    \n",
    "    #with tpu_strategy.scope():\n",
    "    model = Model(inputs=[en_input1, en_input2, en_input3, de_input4, de_input5, de_input6, de_input7], outputs=final_output)\n",
    "    model.compile( optimizer = 'adam',\n",
    "                    loss = 'binary_crossentropy',\n",
    "                    metrics=['accuracy',AUC()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "saint_model2 = build(num_layers, d_model, num_heads, dff, n_question, n_tag, n_part, n_answer, n_prev_q_exp, pe_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:40:05.886239Z",
     "iopub.status.busy": "2020-12-27T02:40:05.869392Z",
     "iopub.status.idle": "2020-12-27T02:40:06.348971Z",
     "shell.execute_reply": "2020-12-27T02:40:06.348118Z"
    },
    "papermill": {
     "duration": 0.527714,
     "end_time": "2020-12-27T02:40:06.349080",
     "exception": false,
     "start_time": "2020-12-27T02:40:05.821366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "saint_model2.load_weights('../input/saint-model2/SAINT_model_feature_extraction.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02482,
     "end_time": "2020-12-27T02:40:06.399499",
     "exception": false,
     "start_time": "2020-12-27T02:40:06.374679",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:40:06.468648Z",
     "iopub.status.busy": "2020-12-27T02:40:06.456386Z",
     "iopub.status.idle": "2020-12-27T02:40:06.802237Z",
     "shell.execute_reply": "2020-12-27T02:40:06.802919Z"
    },
    "papermill": {
     "duration": 0.379154,
     "end_time": "2020-12-27T02:40:06.803071",
     "exception": false,
     "start_time": "2020-12-27T02:40:06.423917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "other_input_dim = 2\n",
    "hidden_layer_size = 128\n",
    "input_dim_order = 13523\n",
    "prev_q_perform_dim = 27046\n",
    "prev_t_perform_dim = 3041\n",
    "prev_p_perform_dim = 15\n",
    "prev_q_exp_dim = 3\n",
    "\n",
    "\n",
    "def dkt_build(hidden_layer_size, input_dim_order, prev_q_perform_dim, prev_t_perform_dim, prev_p_perform_dim, other_input_dim, prev_q_exp_dim):    \n",
    "    masking_func = lambda inputs, previous_mask: previous_mask\n",
    "    ## One hot encode question_id and answer/tag_id and answer\n",
    "    #prev_q_a = Input(batch_shape = (None, None), dtype = 'int32', name = 'prev_q_a')\n",
    "    #one_hot_prev_q_a = tf.one_hot(prev_q_a, prev_q_perform_dim, axis = -1)\n",
    "    \n",
    "    prev_t_a = Input(batch_shape = (None, None), dtype = 'int32', name = 'prev_t_a')\n",
    "    one_hot_prev_t_a = tf.one_hot(prev_t_a, prev_t_perform_dim, axis = -1)\n",
    "    \n",
    "    prev_p_a = Input(batch_shape = (None, None), dtype = 'int32', name = 'prev_p_a')\n",
    "    one_hot_prev_p_a = tf.one_hot(prev_p_a, prev_p_perform_dim, axis = -1)\n",
    "    \n",
    "    prev_q_exp = Input(batch_shape = (None, None), dtype = 'int32', name = 'prev_q_exp')\n",
    "    one_hot_prev_q_exp = tf.one_hot(prev_q_exp, prev_q_exp_dim, axis = -1)\n",
    "    \n",
    "    other_input = Input(batch_shape = (None, None, other_input_dim), name= 'other_input')\n",
    "    \n",
    "    one_hot = Concatenate()([one_hot_prev_t_a, one_hot_prev_p_a, one_hot_prev_q_exp, other_input])\n",
    "    \n",
    "    masked_oh = (Masking(mask_value= 0, input_shape = (None, None, prev_t_perform_dim + prev_p_perform_dim + prev_q_exp + other_input_dim)))(one_hot)\n",
    "    \n",
    "    lstm_out = LSTM(hidden_layer_size, input_shape = (None, None, prev_t_perform_dim + prev_p_perform_dim + other_input_dim + prev_q_exp_dim),\n",
    "                    dropout=0.2, recurrent_dropout =0.2, return_sequences = True)(masked_oh)\n",
    "    \n",
    "    \n",
    "    dense_out = Dense(input_dim_order, input_shape = (None, None, hidden_layer_size), activation='sigmoid')(lstm_out)\n",
    "    order = Input(batch_shape = (None, None), dtype = 'int32', name = 'order')\n",
    "    #one hot encode\n",
    "    one_hot_order = tf.one_hot(order, input_dim_order, axis = -1)\n",
    "    #one_hot_order = K.sum(one_hot_order, axis = -2)\n",
    "    \n",
    "    merged = multiply([dense_out, one_hot_order])\n",
    "    \n",
    "    def reduce_dim(x):\n",
    "        x = K.max(x, axis = 2, keepdims = True)\n",
    "        return x\n",
    "\n",
    "    def reduce_dim_shape(input_shape):\n",
    "        shape = list(input_shape)\n",
    "        shape[-1] = 1\n",
    "        print (\"reduced_shape\", shape)\n",
    "        return tuple(shape)\n",
    "    \n",
    "    reduced = Lambda(reduce_dim, output_shape = reduce_dim_shape, mask = masking_func)(merged)\n",
    "    \n",
    "    model = Model(inputs=[prev_t_a, prev_p_a, prev_q_exp, other_input, order], outputs=reduced)\n",
    "    model.compile( optimizer = 'adam',\n",
    "                    loss = 'binary_crossentropy',\n",
    "                    metrics=['accuracy',AUC()])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "dkt_model = dkt_build(hidden_layer_size, input_dim_order, prev_q_perform_dim, prev_t_perform_dim, prev_p_perform_dim, other_input_dim, prev_q_exp_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:40:06.870498Z",
     "iopub.status.busy": "2020-12-27T02:40:06.869629Z",
     "iopub.status.idle": "2020-12-27T02:40:07.191343Z",
     "shell.execute_reply": "2020-12-27T02:40:07.190244Z"
    },
    "papermill": {
     "duration": 0.361498,
     "end_time": "2020-12-27T02:40:07.191472",
     "exception": false,
     "start_time": "2020-12-27T02:40:06.829974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dkt_model.load_weights('../input/dkt-model/dkt_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025587,
     "end_time": "2020-12-27T02:40:07.242171",
     "exception": false,
     "start_time": "2020-12-27T02:40:07.216584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:40:07.296338Z",
     "iopub.status.busy": "2020-12-27T02:40:07.295695Z",
     "iopub.status.idle": "2020-12-27T02:40:08.108626Z",
     "shell.execute_reply": "2020-12-27T02:40:08.109553Z"
    },
    "papermill": {
     "duration": 0.842081,
     "end_time": "2020-12-27T02:40:08.109724",
     "exception": false,
     "start_time": "2020-12-27T02:40:07.267643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# load model\n",
    "lgbm_model = joblib.load('../input/lgbm-model/lgb_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035707,
     "end_time": "2020-12-27T02:40:08.178360",
     "exception": false,
     "start_time": "2020-12-27T02:40:08.142653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:40:08.248977Z",
     "iopub.status.busy": "2020-12-27T02:40:08.248250Z",
     "iopub.status.idle": "2020-12-27T02:40:08.250107Z",
     "shell.execute_reply": "2020-12-27T02:40:08.249552Z"
    },
    "papermill": {
     "duration": 0.040324,
     "end_time": "2020-12-27T02:40:08.250214",
     "exception": false,
     "start_time": "2020-12-27T02:40:08.209890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TARGET = 'answered_correctly'\n",
    "FEATS = ['answered_correctly_avg_u', 'last_60', 'part_last_30', 'part_answered_correctly_avg_u', 'answered_correctly_avg_c','answered_correctly_std_c','prior_has_explanation_average_c', 'time_diff_average_c'\n",
    "         ,'count_u','part_answered_correctly_sum_u', 'part', 'prior_question_elapsed_time','time_diff','tag_sum','user_prev_tag_lag1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024924,
     "end_time": "2020-12-27T02:40:08.306416",
     "exception": false,
     "start_time": "2020-12-27T02:40:08.281492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Have a fun with loops! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:40:08.375178Z",
     "iopub.status.busy": "2020-12-27T02:40:08.374626Z",
     "iopub.status.idle": "2020-12-27T02:41:12.525554Z",
     "shell.execute_reply": "2020-12-27T02:41:12.524450Z"
    },
    "papermill": {
     "duration": 64.194523,
     "end_time": "2020-12-27T02:41:12.525686",
     "exception": false,
     "start_time": "2020-12-27T02:40:08.331163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "loaded_dictionary = open(\"../input/for-lgbm/last_time_u_dict.pkl\", \"rb\")\n",
    "last_time_u_dict = pickle.load(loaded_dictionary)\n",
    "\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part1_answered_correctly_sum_u_dict.pkl\", \"rb\")\n",
    "part1_answered_correctly_sum_u_dict = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part1_count_u_dict.pkl\", \"rb\")\n",
    "part1_count_u_dict = pickle.load(loaded_dictionary)\n",
    "\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part2_answered_correctly_sum_u_dict.pkl\", \"rb\")\n",
    "part2_answered_correctly_sum_u_dict = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part2_count_u_dict.pkl\", \"rb\")\n",
    "part2_count_u_dict = pickle.load(loaded_dictionary)\n",
    "\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part3_answered_correctly_sum_u_dict.pkl\", \"rb\")\n",
    "part3_answered_correctly_sum_u_dict = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part3_count_u_dict.pkl\", \"rb\")\n",
    "part3_count_u_dict = pickle.load(loaded_dictionary)\n",
    "\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part4_answered_correctly_sum_u_dict.pkl\", \"rb\")\n",
    "part4_answered_correctly_sum_u_dict = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part4_count_u_dict.pkl\", \"rb\")\n",
    "part4_count_u_dict = pickle.load(loaded_dictionary)\n",
    "\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part5_answered_correctly_sum_u_dict.pkl\", \"rb\")\n",
    "part5_answered_correctly_sum_u_dict = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part5_count_u_dict.pkl\", \"rb\")\n",
    "part5_count_u_dict = pickle.load(loaded_dictionary)\n",
    "\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part6_answered_correctly_sum_u_dict.pkl\", \"rb\")\n",
    "part6_answered_correctly_sum_u_dict = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part6_count_u_dict.pkl\", \"rb\")\n",
    "part6_count_u_dict = pickle.load(loaded_dictionary)\n",
    "\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part7_answered_correctly_sum_u_dict.pkl\", \"rb\")\n",
    "part7_answered_correctly_sum_u_dict = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part7_count_u_dict.pkl\", \"rb\")\n",
    "part7_count_u_dict = pickle.load(loaded_dictionary)\n",
    "\n",
    "loaded_dictionary = open(\"../input/for-lgbm/user_answer_lag1.pkl\", \"rb\")\n",
    "user_answer_lag1 = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/for-lgbm/user_tag_lag1.pkl\", \"rb\")\n",
    "user_tag_lag1 = pickle.load(loaded_dictionary)\n",
    "\n",
    "loaded_dictionary = open(\"../input/for-lgbm/all_last60_avg.pkl\", \"rb\")\n",
    "all_last60_avg = pickle.load(loaded_dictionary)\n",
    "\n",
    "\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part1_last30_avg.pkl\", \"rb\")\n",
    "part1_last30_avg = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part2_last30_avg.pkl\", \"rb\")\n",
    "part2_last30_avg = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part3_last30_avg.pkl\", \"rb\")\n",
    "part3_last30_avg = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part4_last30_avg.pkl\", \"rb\")\n",
    "part4_last30_avg = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part5_last30_avg.pkl\", \"rb\")\n",
    "part5_last30_avg = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part6_last30_avg.pkl\", \"rb\")\n",
    "part6_last30_avg = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/for-lgbm/part7_last30_avg.pkl\", \"rb\")\n",
    "part7_last30_avg = pickle.load(loaded_dictionary)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loaded_dictionary = open(\"../input/prev-user-q-a/past_answer.pkl\", \"rb\")\n",
    "past_answer = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/prev-user-q-a/past_part.pkl\", \"rb\")\n",
    "past_part = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/prev-user-q-a/past_prior_elaps.pkl\", \"rb\")\n",
    "past_prior_elaps = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/prev-user-q-a/past_question.pkl\", \"rb\")\n",
    "past_question = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/prev-user-q-a/past_tag.pkl\", \"rb\")\n",
    "past_tag = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/prev-user-q-a/past_time_diff.pkl\", \"rb\")\n",
    "past_time_diff = pickle.load(loaded_dictionary)\n",
    "loaded_dictionary = open(\"../input/prev-user-q-a/past_prior_exp.pkl\", \"rb\")\n",
    "past_prior_exp = pickle.load(loaded_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:41:12.600492Z",
     "iopub.status.busy": "2020-12-27T02:41:12.598404Z",
     "iopub.status.idle": "2020-12-27T02:41:12.601176Z",
     "shell.execute_reply": "2020-12-27T02:41:12.601759Z"
    },
    "papermill": {
     "duration": 0.050701,
     "end_time": "2020-12-27T02:41:12.601895",
     "exception": false,
     "start_time": "2020-12-27T02:41:12.551194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Iter_Valid(object):\n",
    "    def __init__(self, df, max_user=1000):\n",
    "        df = df.reset_index(drop=True)\n",
    "        self.df = df\n",
    "        self.user_answer = df['user_answer'].astype(str).values\n",
    "        self.answered_correctly = df['answered_correctly'].astype(str).values\n",
    "        df['prior_group_responses'] = \"[]\"\n",
    "        df['prior_group_answers_correct'] = \"[]\"\n",
    "        self.sample_df = df[df['content_type_id'] == 0][['row_id']]\n",
    "        self.sample_df['answered_correctly'] = 0\n",
    "        self.len = len(df)\n",
    "        self.user_id = df.user_id.values\n",
    "        self.task_container_id = df.task_container_id.values\n",
    "        self.content_type_id = df.content_type_id.values\n",
    "        self.max_user = max_user\n",
    "        self.current = 0\n",
    "        self.pre_user_answer_list = []\n",
    "        self.pre_answered_correctly_list = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def fix_df(self, user_answer_list, answered_correctly_list, pre_start):\n",
    "        df= self.df[pre_start:self.current].copy()\n",
    "        sample_df = self.sample_df[pre_start:self.current].copy()\n",
    "        df.loc[pre_start,'prior_group_responses'] = '[' + \",\".join(self.pre_user_answer_list) + ']'\n",
    "        df.loc[pre_start,'prior_group_answers_correct'] = '[' + \",\".join(self.pre_answered_correctly_list) + ']'\n",
    "        self.pre_user_answer_list = user_answer_list\n",
    "        self.pre_answered_correctly_list = answered_correctly_list\n",
    "        return df, sample_df\n",
    "\n",
    "    def __next__(self):\n",
    "        added_user = set()\n",
    "        pre_start = self.current\n",
    "        pre_added_user = -1\n",
    "        pre_task_container_id = -1\n",
    "        pre_content_type_id = -1\n",
    "        user_answer_list = []\n",
    "        answered_correctly_list = []\n",
    "        while self.current < self.len:\n",
    "            crr_user_id = self.user_id[self.current]\n",
    "            crr_task_container_id = self.task_container_id[self.current]\n",
    "            crr_content_type_id = self.content_type_id[self.current]\n",
    "            if crr_user_id in added_user and (crr_user_id != pre_added_user or (crr_task_container_id != pre_task_container_id and crr_content_type_id == 0 and pre_content_type_id == 0)):\n",
    "                # known user(not prev user or (differnt task container and both question))\n",
    "                return self.fix_df(user_answer_list, answered_correctly_list, pre_start)\n",
    "            if len(added_user) == self.max_user:\n",
    "                if  crr_user_id == pre_added_user and (crr_task_container_id == pre_task_container_id or crr_content_type_id == 1):\n",
    "                    user_answer_list.append(self.user_answer[self.current])\n",
    "                    answered_correctly_list.append(self.answered_correctly[self.current])\n",
    "                    self.current += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    return self.fix_df(user_answer_list, answered_correctly_list, pre_start)\n",
    "            added_user.add(crr_user_id)\n",
    "            pre_added_user = crr_user_id\n",
    "            pre_task_container_id = crr_task_container_id\n",
    "            pre_content_type_id = crr_content_type_id\n",
    "            user_answer_list.append(self.user_answer[self.current])\n",
    "            answered_correctly_list.append(self.answered_correctly[self.current])\n",
    "            self.current += 1\n",
    "        if pre_start < self.current:\n",
    "            return self.fix_df(user_answer_list, answered_correctly_list, pre_start)\n",
    "        else:\n",
    "            raise StopIteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:41:12.670994Z",
     "iopub.status.busy": "2020-12-27T02:41:12.670406Z",
     "iopub.status.idle": "2020-12-27T02:41:12.689651Z",
     "shell.execute_reply": "2020-12-27T02:41:12.689213Z"
    },
    "papermill": {
     "duration": 0.057606,
     "end_time": "2020-12-27T02:41:12.689742",
     "exception": false,
     "start_time": "2020-12-27T02:41:12.632136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can debug your inference code to reduce \"Submission Scoring Error\" with `validaten_flg = True`.\n",
    "# Please refer https://www.kaggle.com/its7171/time-series-api-iter-test-emulator about Time-series API (iter_test) Emulator.\n",
    "\n",
    "if validaten_flg:\n",
    "    target_df = pd.read_pickle(valid_pickle)\n",
    "    if debug:\n",
    "        target_df = target_df[:10000]\n",
    "    iter_test = Iter_Valid(target_df,max_user=1000)\n",
    "    predicted = []\n",
    "    def set_predict(df):\n",
    "        predicted.append(df)\n",
    "    # reset answered_correctly_sum_u_dict and count_u_dict\n",
    "    part1_answered_correctly_sum_u_dict = defaultdict(int)\n",
    "    part1_count_u_dict = defaultdict(int)\n",
    "    part2_answered_correctly_sum_u_dict = defaultdict(int)\n",
    "    part2_count_u_dict = defaultdict(int)\n",
    "    part3_answered_correctly_sum_u_dict = defaultdict(int)\n",
    "    part3_count_u_dict = defaultdict(int)\n",
    "    part4_answered_correctly_sum_u_dict = defaultdict(int)\n",
    "    part4_count_u_dict = defaultdict(int)\n",
    "    part5_answered_correctly_sum_u_dict = defaultdict(int)\n",
    "    part5_count_u_dict = defaultdict(int)\n",
    "    part6_answered_correctly_sum_u_dict = defaultdict(int)\n",
    "    part6_count_u_dict = defaultdict(int)\n",
    "    part7_answered_correctly_sum_u_dict = defaultdict(int)\n",
    "    part7_count_u_dict = defaultdict(int)\n",
    "    train = pd.read_pickle(train_pickle)[['user_id','answered_correctly','content_type_id','content_id']]\n",
    "    if debug:\n",
    "        train = train[:1000000]\n",
    "    train = train[train.content_type_id == False].reset_index(drop=True)\n",
    "    train = train.merge(questions_df[['question_id','part']],left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "    update_user_feats(train, part1_answered_correctly_sum_u_dict, part1_count_u_dict, part2_answered_correctly_sum_u_dict, part2_count_u_dict, part3_answered_correctly_sum_u_dict, part3_count_u_dict, part4_answered_correctly_sum_u_dict, part4_count_u_dict, part5_answered_correctly_sum_u_dict, part5_count_u_dict, part6_answered_correctly_sum_u_dict, part6_count_u_dict, part7_answered_correctly_sum_u_dict, part7_count_u_dict)\n",
    "    del train\n",
    "else:\n",
    "    import riiideducation\n",
    "    env = riiideducation.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    set_predict = env.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:41:12.754296Z",
     "iopub.status.busy": "2020-12-27T02:41:12.753484Z",
     "iopub.status.idle": "2020-12-27T02:41:19.817839Z",
     "shell.execute_reply": "2020-12-27T02:41:19.816336Z"
    },
    "papermill": {
     "duration": 7.102871,
     "end_time": "2020-12-27T02:41:19.817960",
     "exception": false,
     "start_time": "2020-12-27T02:41:12.715089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "100%|| 18/18 [00:00<00:00, 9676.68it/s]\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "100%|| 27/27 [00:00<00:00, 10287.63it/s]\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "100%|| 26/26 [00:00<00:00, 11093.78it/s]\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "100%|| 33/33 [00:00<00:00, 11249.35it/s]\n"
     ]
    }
   ],
   "source": [
    "previous_test_df = None\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    if previous_test_df is not None:\n",
    "        previous_test_df[TARGET] = np.array(eval(test_df[\"prior_group_answers_correct\"].iloc[0]))[mask]\n",
    "        update_user_feats(previous_test_df , all_last60_avg, part1_last30_avg, part2_last30_avg, part3_last30_avg\n",
    "                      , part4_last30_avg, part5_last30_avg, part6_last30_avg, part7_last30_avg , user_tag_lag1, user_answer_lag1 ,\n",
    "                      last_time_u_dict, part1_answered_correctly_sum_u_dict, part1_count_u_dict, \n",
    "                      part2_answered_correctly_sum_u_dict, part2_count_u_dict, part3_answered_correctly_sum_u_dict, part3_count_u_dict,\n",
    "                      part4_answered_correctly_sum_u_dict, part4_count_u_dict, part5_answered_correctly_sum_u_dict, part5_count_u_dict,\n",
    "                      part6_answered_correctly_sum_u_dict, part6_count_u_dict, part7_answered_correctly_sum_u_dict, part7_count_u_dict, \n",
    "                      past_question, past_tag, past_part, past_answer, past_prior_elaps, past_time_diff, past_prior_exp)\n",
    "    test_df = pd.merge(test_df, questions_df[['content_id', 'part','tag_sum','tag_num']], on='content_id', how='left')\n",
    "    mask = (test_df['content_type_id'] == 0).values.tolist()\n",
    "    test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop=True)\n",
    "    test_df = add_user_feats_without_update(test_df , all_last60_avg, part1_last30_avg, part2_last30_avg, part3_last30_avg, \n",
    "                                  part4_last30_avg, part5_last30_avg, part6_last30_avg, part7_last30_avg, user_tag_lag1, user_answer_lag1,\n",
    "                                  last_time_u_dict, part1_answered_correctly_sum_u_dict, part1_count_u_dict, part2_answered_correctly_sum_u_dict,\n",
    "                                  part2_count_u_dict, part3_answered_correctly_sum_u_dict, part3_count_u_dict, part4_answered_correctly_sum_u_dict,\n",
    "                                  part4_count_u_dict, part5_answered_correctly_sum_u_dict, part5_count_u_dict, part6_answered_correctly_sum_u_dict,\n",
    "                                  part6_count_u_dict, part7_answered_correctly_sum_u_dict, part7_count_u_dict)\n",
    "    test_df.time_diff.loc[test_df.time_diff >= 1e6] = 1e6    \n",
    "    test_df = pd.merge(test_df, content_df, on='content_id',  how=\"left\")\n",
    "    test_df['prior_question_elapsed_time'] = test_df.prior_question_elapsed_time.fillna(prior_question_elapsed_time_mean)    \n",
    "    test_df['prior_question_had_explanation'] = test_df.prior_question_had_explanation*1 + 1\n",
    "    test_df['prior_question_had_explanation'].fillna(3, inplace = True)\n",
    "    \n",
    "    previous_test_df = test_df[['user_id','timestamp','content_type_id','content_id','part','tag_sum','tag_num','prior_question_elapsed_time','time_diff', 'prior_question_had_explanation']].copy()\n",
    "    \n",
    "    lgbm_predict = lgbm_model.predict(test_df[FEATS])\n",
    "    \n",
    "    current_part, current_tag, current_question, past_part_answer, past_tag_answer, past_question_answer, past_other_feats, past_answer_correctly, past_prior_explanation = get_user_feats_for_nn_without_update(test_df, past_question, past_tag, past_part, past_answer, past_prior_elaps, past_time_diff, past_prior_exp)\n",
    "    past_other_feats1 = np.reshape(past_other_feats[:,:,0],(-1,60,1))\n",
    "    past_other_feats2 = np.reshape(past_other_feats[:,:,1],(-1,60,1))\n",
    "    \n",
    "    #transformer_predict = transformer_model.predict([past_question_answer, past_tag_answer, past_part_answer,\n",
    "    #                           current_question, current_tag, current_part,\n",
    "    #                           past_other_feats1, past_other_feats2, past_prior_explanation], batch_size = 500)\n",
    "    \n",
    "    \n",
    "    #sakt_predict = sakt_model.predict([past_answer_correctly, past_other_feats1, past_other_feats2,past_prior_explanation,\n",
    "    #                           current_question, current_tag, current_part], batch_size = 500)\n",
    "    \n",
    "    saint1_predict = saint_model1.predict([current_question, current_tag, current_part, \n",
    "                                past_answer_correctly, past_other_feats1, past_other_feats2, past_prior_explanation], batch_size = 500)\n",
    "    \n",
    "    saint2_predict = saint_model2.predict([current_question, current_tag, current_part, \n",
    "                                past_answer_correctly, past_other_feats1, past_other_feats2, past_prior_explanation], batch_size = 500)\n",
    "    \n",
    "    past_tag_answer = past_tag_answer - 1\n",
    "    past_part_answer = past_part_answer - 1\n",
    "    past_question_answer = past_question_answer - 1\n",
    "    current_question = current_question - 1\n",
    "    past_prior_explanation = past_prior_explanation - 1\n",
    "    \n",
    "    dkt_predict = dkt_model.predict([past_tag_answer, past_part_answer, past_prior_explanation, past_other_feats, current_question], batch_size = 100)\n",
    "    test_df[TARGET] = dkt_predict[:,-1,0]*(0.0067) + saint1_predict[:,-1,0]*(0.5654)  + lgbm_predict*0.2491 + saint2_predict[:,-1,0]*0.1788\n",
    "    test_df[TARGET].fillna(0.65, inplace = True)\n",
    "    set_predict(test_df[['row_id', TARGET]])\n",
    "    #---\n",
    "    #print(sample_prediction_df)\n",
    "    #print(test_df[['row_id', TARGET]])\n",
    "    #print(test_df.shape, sample_prediction_df.shape, test_df[TARGET].shape)\n",
    "    #---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-27T02:41:19.882505Z",
     "iopub.status.busy": "2020-12-27T02:41:19.880913Z",
     "iopub.status.idle": "2020-12-27T02:41:19.883362Z",
     "shell.execute_reply": "2020-12-27T02:41:19.883783Z"
    },
    "papermill": {
     "duration": 0.036261,
     "end_time": "2020-12-27T02:41:19.883891",
     "exception": false,
     "start_time": "2020-12-27T02:41:19.847630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if validaten_flg:\n",
    "    y_true = target_df[target_df.content_type_id == 0].answered_correctly\n",
    "    y_pred = pd.concat(predicted).answered_correctly\n",
    "    print(roc_auc_score(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 200.870274,
   "end_time": "2020-12-27T02:41:21.783882",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-27T02:38:00.913608",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
