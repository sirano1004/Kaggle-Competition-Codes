{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02136,
     "end_time": "2020-12-15T11:31:29.583096",
     "exception": false,
     "start_time": "2020-12-15T11:31:29.561736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I want to convey two things in this notebook.\n",
    "## 1. Don't have to be hesitant about using Loop.\n",
    "They say \"avoid loops!'.\n",
    "But I think It's not bad idea to use loops for this competition.\n",
    "Because:\n",
    "* We have to use small batch inference using Time-series API.\n",
    "* Loops have very small overhead for each batch.\n",
    "* Loops are more flexible.\n",
    "* Even loops are not so slow. 3 features are extracted within 10 minits for 100M train data, as you can see blow.\n",
    "\n",
    "## 2. Future information should not be used.\n",
    "Time-series API doesn't allow us to use information from the future.\n",
    "So we should not use it, especially user statistics from future make things very bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:31:29.628346Z",
     "iopub.status.busy": "2020-12-15T11:31:29.627639Z",
     "iopub.status.idle": "2020-12-15T11:31:30.815976Z",
     "shell.execute_reply": "2020-12-15T11:31:30.815245Z"
    },
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 1.21321,
     "end_time": "2020-12-15T11:31:30.816158",
     "exception": false,
     "start_time": "2020-12-15T11:31:29.602948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict, deque\n",
    "from tqdm.notebook import tqdm\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018753,
     "end_time": "2020-12-15T11:31:30.854031",
     "exception": false,
     "start_time": "2020-12-15T11:31:30.835278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## setting\n",
    "CV files are generated by [this notebook](https://www.kaggle.com/its7171/cv-strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:31:30.895273Z",
     "iopub.status.busy": "2020-12-15T11:31:30.894293Z",
     "iopub.status.idle": "2020-12-15T11:31:30.898835Z",
     "shell.execute_reply": "2020-12-15T11:31:30.899375Z"
    },
    "papermill": {
     "duration": 0.026792,
     "end_time": "2020-12-15T11:31:30.899517",
     "exception": false,
     "start_time": "2020-12-15T11:31:30.872725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_pickle = '../input/riiid-cross-validation-files/cv1_train.pickle'\n",
    "valid_pickle = '../input/riiid-cross-validation-files/cv1_valid.pickle'\n",
    "question_file = '../input/features/question3.csv'\n",
    "debug = False\n",
    "validaten_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018469,
     "end_time": "2020-12-15T11:31:30.936702",
     "exception": false,
     "start_time": "2020-12-15T11:31:30.918233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:31:30.979776Z",
     "iopub.status.busy": "2020-12-15T11:31:30.978584Z",
     "iopub.status.idle": "2020-12-15T11:32:38.377317Z",
     "shell.execute_reply": "2020-12-15T11:32:38.375980Z"
    },
    "papermill": {
     "duration": 67.420786,
     "end_time": "2020-12-15T11:32:38.377477",
     "exception": false,
     "start_time": "2020-12-15T11:31:30.956691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "feld_needed = ['user_id','content_id','answered_correctly','prior_question_elapsed_time', 'prior_question_had_explanation']\n",
    "\n",
    "loaded_dictionary = open(train_pickle, \"rb\")\n",
    "train = pickle.load(loaded_dictionary)\n",
    "train = train[feld_needed]\n",
    "\n",
    "\n",
    "loaded_dictionary = open(valid_pickle, \"rb\")\n",
    "valid = pickle.load(loaded_dictionary)\n",
    "valid = valid[feld_needed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:32:38.423479Z",
     "iopub.status.busy": "2020-12-15T11:32:38.422743Z",
     "iopub.status.idle": "2020-12-15T11:32:43.832439Z",
     "shell.execute_reply": "2020-12-15T11:32:43.831840Z"
    },
    "papermill": {
     "duration": 5.434575,
     "end_time": "2020-12-15T11:32:43.832556",
     "exception": false,
     "start_time": "2020-12-15T11:32:38.397981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.loc[train.answered_correctly != -1].reset_index(drop=True)\n",
    "valid = valid.loc[valid.answered_correctly != -1].reset_index(drop=True)\n",
    "_=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:32:43.880463Z",
     "iopub.status.busy": "2020-12-15T11:32:43.879714Z",
     "iopub.status.idle": "2020-12-15T11:32:44.879213Z",
     "shell.execute_reply": "2020-12-15T11:32:44.878608Z"
    },
    "papermill": {
     "duration": 1.025837,
     "end_time": "2020-12-15T11:32:44.879333",
     "exception": false,
     "start_time": "2020-12-15T11:32:43.853496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prior_question_elapsed_time_mean = train.prior_question_elapsed_time.dropna().values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:32:44.921867Z",
     "iopub.status.busy": "2020-12-15T11:32:44.920956Z",
     "iopub.status.idle": "2020-12-15T11:32:45.138630Z",
     "shell.execute_reply": "2020-12-15T11:32:45.138111Z"
    },
    "papermill": {
     "duration": 0.239938,
     "end_time": "2020-12-15T11:32:45.138758",
     "exception": false,
     "start_time": "2020-12-15T11:32:44.898820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.content_id += 1\n",
    "valid.content_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:32:45.180363Z",
     "iopub.status.busy": "2020-12-15T11:32:45.179715Z",
     "iopub.status.idle": "2020-12-15T11:32:45.262500Z",
     "shell.execute_reply": "2020-12-15T11:32:45.262984Z"
    },
    "papermill": {
     "duration": 0.105079,
     "end_time": "2020-12-15T11:32:45.263143",
     "exception": false,
     "start_time": "2020-12-15T11:32:45.158064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train[-40000000:]\n",
    "_=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:32:45.305290Z",
     "iopub.status.busy": "2020-12-15T11:32:45.304665Z",
     "iopub.status.idle": "2020-12-15T11:33:22.169489Z",
     "shell.execute_reply": "2020-12-15T11:33:22.168175Z"
    },
    "papermill": {
     "duration": 36.88693,
     "end_time": "2020-12-15T11:33:22.169629",
     "exception": false,
     "start_time": "2020-12-15T11:32:45.282699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_user_prev_q_a = pd.read_csv('../input/user-prev-for-saint/train_user_prev_perform.csv')\n",
    "train_user_prev_q_a = train_user_prev_q_a[-40000000:]\n",
    "train_user_prev_q_a.user_prev_answer_lag += 1\n",
    "_=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:33:22.218243Z",
     "iopub.status.busy": "2020-12-15T11:33:22.216729Z",
     "iopub.status.idle": "2020-12-15T11:33:22.780426Z",
     "shell.execute_reply": "2020-12-15T11:33:22.780985Z"
    },
    "papermill": {
     "duration": 0.591816,
     "end_time": "2020-12-15T11:33:22.781134",
     "exception": false,
     "start_time": "2020-12-15T11:33:22.189318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.concat([train,train_user_prev_q_a['user_prev_answer_lag']], axis = 1)\n",
    "del(train_user_prev_q_a)\n",
    "_=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:33:22.823417Z",
     "iopub.status.busy": "2020-12-15T11:33:22.822723Z",
     "iopub.status.idle": "2020-12-15T11:33:23.093222Z",
     "shell.execute_reply": "2020-12-15T11:33:23.093679Z"
    },
    "papermill": {
     "duration": 0.293214,
     "end_time": "2020-12-15T11:33:23.093820",
     "exception": false,
     "start_time": "2020-12-15T11:33:22.800606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                           2147482216\n",
       "content_id                             13523\n",
       "answered_correctly                         1\n",
       "prior_question_elapsed_time           300000\n",
       "prior_question_had_explanation          True\n",
       "user_prev_answer_lag                       3\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:33:23.144289Z",
     "iopub.status.busy": "2020-12-15T11:33:23.143616Z",
     "iopub.status.idle": "2020-12-15T11:33:24.112274Z",
     "shell.execute_reply": "2020-12-15T11:33:24.111518Z"
    },
    "papermill": {
     "duration": 0.997902,
     "end_time": "2020-12-15T11:33:24.112404",
     "exception": false,
     "start_time": "2020-12-15T11:33:23.114502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_user_prev_q_a = pd.read_csv('../input/user-prev-for-saint/valid_user_prev_perform.csv')\n",
    "valid_user_prev_q_a.user_prev_answer_lag += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:33:24.159210Z",
     "iopub.status.busy": "2020-12-15T11:33:24.158426Z",
     "iopub.status.idle": "2020-12-15T11:33:24.185144Z",
     "shell.execute_reply": "2020-12-15T11:33:24.184404Z"
    },
    "papermill": {
     "duration": 0.052693,
     "end_time": "2020-12-15T11:33:24.185286",
     "exception": false,
     "start_time": "2020-12-15T11:33:24.132593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid = pd.concat([valid,valid_user_prev_q_a['user_prev_answer_lag']], axis = 1)\n",
    "del(valid_user_prev_q_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:33:24.245035Z",
     "iopub.status.busy": "2020-12-15T11:33:24.244335Z",
     "iopub.status.idle": "2020-12-15T11:33:40.105079Z",
     "shell.execute_reply": "2020-12-15T11:33:40.104515Z"
    },
    "papermill": {
     "duration": 15.893058,
     "end_time": "2020-12-15T11:33:40.105196",
     "exception": false,
     "start_time": "2020-12-15T11:33:24.212138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_time_diff = pd.read_csv('../input/time-diff-lgbm/train_time_diff.csv')\n",
    "train_time_diff = train_time_diff[-40000000:]\n",
    "_=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:33:40.152139Z",
     "iopub.status.busy": "2020-12-15T11:33:40.151491Z",
     "iopub.status.idle": "2020-12-15T11:33:41.495857Z",
     "shell.execute_reply": "2020-12-15T11:33:41.496417Z"
    },
    "papermill": {
     "duration": 1.370931,
     "end_time": "2020-12-15T11:33:41.496595",
     "exception": false,
     "start_time": "2020-12-15T11:33:40.125664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "train = pd.concat([train,train_time_diff], axis = 1)\n",
    "train.time_diff.loc[train.time_diff >= 1e6] = 1e6\n",
    "del(train_time_diff)\n",
    "_=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:33:41.550265Z",
     "iopub.status.busy": "2020-12-15T11:33:41.549483Z",
     "iopub.status.idle": "2020-12-15T11:33:41.943833Z",
     "shell.execute_reply": "2020-12-15T11:33:41.942723Z"
    },
    "papermill": {
     "duration": 0.426445,
     "end_time": "2020-12-15T11:33:41.944014",
     "exception": false,
     "start_time": "2020-12-15T11:33:41.517569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_time_diff = pd.read_csv('../input/time-diff-lgbm/valid_time_diff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:33:41.992929Z",
     "iopub.status.busy": "2020-12-15T11:33:41.992225Z",
     "iopub.status.idle": "2020-12-15T11:33:42.069225Z",
     "shell.execute_reply": "2020-12-15T11:33:42.069707Z"
    },
    "papermill": {
     "duration": 0.104826,
     "end_time": "2020-12-15T11:33:42.069865",
     "exception": false,
     "start_time": "2020-12-15T11:33:41.965039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "valid = pd.concat([valid,valid_time_diff], axis = 1)\n",
    "valid.time_diff.loc[valid.time_diff >= 1e6] = 1e6\n",
    "del(valid_time_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:33:42.116334Z",
     "iopub.status.busy": "2020-12-15T11:33:42.115643Z",
     "iopub.status.idle": "2020-12-15T11:33:50.642537Z",
     "shell.execute_reply": "2020-12-15T11:33:50.641991Z"
    },
    "papermill": {
     "duration": 8.550842,
     "end_time": "2020-12-15T11:33:50.642648",
     "exception": false,
     "start_time": "2020-12-15T11:33:42.091806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = pd.read_csv('../input/features/question3.csv')\n",
    "question.tags.fillna('-1-1', inplace = True)\n",
    "question['tag_num'] = pd.factorize(question.tags)[0]\n",
    "\n",
    "question.content_id += 1\n",
    "question.tag_num += 1\n",
    "\n",
    "train = train.merge(question[['content_id','part','tag_num']], on = 'content_id', how = 'left')\n",
    "valid = valid.merge(question[['content_id','part','tag_num']], on = 'content_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:33:50.690769Z",
     "iopub.status.busy": "2020-12-15T11:33:50.690011Z",
     "iopub.status.idle": "2020-12-15T11:33:50.693122Z",
     "shell.execute_reply": "2020-12-15T11:33:50.692509Z"
    },
    "papermill": {
     "duration": 0.02912,
     "end_time": "2020-12-15T11:33:50.693237",
     "exception": false,
     "start_time": "2020-12-15T11:33:50.664117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FEATS = ['prior_question_elapsed_time','time_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:33:50.744314Z",
     "iopub.status.busy": "2020-12-15T11:33:50.743590Z",
     "iopub.status.idle": "2020-12-15T11:33:51.419486Z",
     "shell.execute_reply": "2020-12-15T11:33:51.418870Z"
    },
    "papermill": {
     "duration": 0.704896,
     "end_time": "2020-12-15T11:33:51.419604",
     "exception": false,
     "start_time": "2020-12-15T11:33:50.714708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.prior_question_elapsed_time.fillna(prior_question_elapsed_time_mean, inplace = True)\n",
    "valid.prior_question_elapsed_time.fillna(prior_question_elapsed_time_mean, inplace = True)\n",
    "\n",
    "train.prior_question_had_explanation = train.prior_question_had_explanation*1 + 1\n",
    "valid.prior_question_had_explanation = valid.prior_question_had_explanation*1 + 1\n",
    "\n",
    "train.prior_question_had_explanation.fillna(3, inplace = True)\n",
    "valid.prior_question_had_explanation.fillna(3, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:33:51.485718Z",
     "iopub.status.busy": "2020-12-15T11:33:51.484177Z",
     "iopub.status.idle": "2020-12-15T11:33:51.997438Z",
     "shell.execute_reply": "2020-12-15T11:33:51.996933Z"
    },
    "papermill": {
     "duration": 0.555956,
     "end_time": "2020-12-15T11:33:51.997562",
     "exception": false,
     "start_time": "2020-12-15T11:33:51.441606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.prior_question_elapsed_time = train.prior_question_elapsed_time/300000\n",
    "train.time_diff = train.time_diff/1e6\n",
    "\n",
    "valid.prior_question_elapsed_time = valid.prior_question_elapsed_time/300000 \n",
    "valid.time_diff = valid.time_diff/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:33:52.048927Z",
     "iopub.status.busy": "2020-12-15T11:33:52.048213Z",
     "iopub.status.idle": "2020-12-15T11:34:06.999339Z",
     "shell.execute_reply": "2020-12-15T11:34:06.998763Z"
    },
    "papermill": {
     "duration": 14.978809,
     "end_time": "2020-12-15T11:34:06.999460",
     "exception": false,
     "start_time": "2020-12-15T11:33:52.020651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_user_count = train.user_id.value_counts()\n",
    "train_del_user = train_user_count[train_user_count<30]\n",
    "train = train[~train.user_id.isin(train_del_user.index)]\n",
    "\n",
    "valid_user_count = valid.user_id.value_counts()\n",
    "valid_del_user = valid_user_count[valid_user_count<30]\n",
    "valid = valid[~valid.user_id.isin(valid_del_user.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:34:07.049589Z",
     "iopub.status.busy": "2020-12-15T11:34:07.048992Z",
     "iopub.status.idle": "2020-12-15T11:34:13.114760Z",
     "shell.execute_reply": "2020-12-15T11:34:13.114222Z"
    },
    "papermill": {
     "duration": 6.092548,
     "end_time": "2020-12-15T11:34:13.114911",
     "exception": false,
     "start_time": "2020-12-15T11:34:07.022363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_len = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:34:13.309119Z",
     "iopub.status.busy": "2020-12-15T11:34:13.308441Z",
     "iopub.status.idle": "2020-12-15T11:34:13.311701Z",
     "shell.execute_reply": "2020-12-15T11:34:13.311177Z"
    },
    "papermill": {
     "duration": 0.174838,
     "end_time": "2020-12-15T11:34:13.311812",
     "exception": false,
     "start_time": "2020-12-15T11:34:13.136974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_group = train.groupby('user_id')\n",
    "del(train)\n",
    "_ = gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:34:13.364839Z",
     "iopub.status.busy": "2020-12-15T11:34:13.363372Z",
     "iopub.status.idle": "2020-12-15T11:41:16.378370Z",
     "shell.execute_reply": "2020-12-15T11:41:16.379047Z"
    },
    "papermill": {
     "duration": 423.045479,
     "end_time": "2020-12-15T11:41:16.379310",
     "exception": false,
     "start_time": "2020-12-15T11:34:13.333831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_y = [frame['answered_correctly'].to_numpy()[:, None].tolist()[i:i+max_len] for _, frame in train_group\n",
    "           for i in range(0, len(frame['answered_correctly'].to_numpy()[:, None]), max_len) ]\n",
    "\n",
    "train_y = np.reshape(pad_sequences(train_y, padding=\"pre\"),(-1,max_len,1))\n",
    "\n",
    "\n",
    "f = open(\"train_y.pkl\",\"wb\")\n",
    "pickle.dump(train_y,f)\n",
    "f.close()\n",
    "\n",
    "del(train_y)\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:41:16.433060Z",
     "iopub.status.busy": "2020-12-15T11:41:16.432384Z",
     "iopub.status.idle": "2020-12-15T11:48:51.581460Z",
     "shell.execute_reply": "2020-12-15T11:48:51.580784Z"
    },
    "papermill": {
     "duration": 455.178551,
     "end_time": "2020-12-15T11:48:51.581608",
     "exception": false,
     "start_time": "2020-12-15T11:41:16.403057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_current_question = [frame['content_id'].to_numpy()[:, None].tolist()[i:i+max_len] for _, frame in train_group\n",
    "                         for i in range(0, len(frame['content_id'].to_numpy()[:,None]), max_len )]\n",
    "\n",
    "train_current_question = np.reshape(pad_sequences(train_current_question, padding=\"pre\"),(-1,max_len))\n",
    "\n",
    "\n",
    "f = open(\"train_current_question.pkl\",\"wb\")\n",
    "pickle.dump(train_current_question,f)\n",
    "f.close()\n",
    "\n",
    "del(train_current_question)\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:48:51.635912Z",
     "iopub.status.busy": "2020-12-15T11:48:51.635245Z",
     "iopub.status.idle": "2020-12-15T11:56:37.474107Z",
     "shell.execute_reply": "2020-12-15T11:56:37.473356Z"
    },
    "papermill": {
     "duration": 465.868418,
     "end_time": "2020-12-15T11:56:37.474262",
     "exception": false,
     "start_time": "2020-12-15T11:48:51.605844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_current_tag = [frame['tag_num'].to_numpy()[:, None].tolist()[i:i+max_len] for _, frame in train_group\n",
    "                         for i in range(0, len(frame['tag_num'].to_numpy()[:,None]), max_len )] \n",
    "\n",
    "train_current_tag = np.reshape(pad_sequences(train_current_tag, padding=\"pre\"),(-1,max_len))\n",
    "\n",
    "f = open(\"train_current_tag.pkl\",\"wb\")\n",
    "pickle.dump(train_current_tag,f)\n",
    "f.close()\n",
    "\n",
    "del(train_current_tag)\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T11:56:37.529124Z",
     "iopub.status.busy": "2020-12-15T11:56:37.528440Z",
     "iopub.status.idle": "2020-12-15T12:03:36.015503Z",
     "shell.execute_reply": "2020-12-15T12:03:36.016008Z"
    },
    "papermill": {
     "duration": 418.517458,
     "end_time": "2020-12-15T12:03:36.016211",
     "exception": false,
     "start_time": "2020-12-15T11:56:37.498753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_current_part = [frame['part'].to_numpy()[:, None].tolist()[i:i+max_len] for _, frame in train_group\n",
    "                         for i in range(0, len(frame['part'].to_numpy()[:,None]), max_len )] \n",
    "train_current_part = np.reshape(pad_sequences(train_current_part, padding=\"pre\"),(-1,max_len))\n",
    "\n",
    "\n",
    "f = open(\"train_current_part.pkl\",\"wb\")\n",
    "pickle.dump(train_current_part,f)\n",
    "f.close()\n",
    "\n",
    "del(train_current_part)\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:03:36.069428Z",
     "iopub.status.busy": "2020-12-15T12:03:36.068740Z",
     "iopub.status.idle": "2020-12-15T12:10:22.827988Z",
     "shell.execute_reply": "2020-12-15T12:10:22.827377Z"
    },
    "papermill": {
     "duration": 406.788804,
     "end_time": "2020-12-15T12:10:22.828143",
     "exception": false,
     "start_time": "2020-12-15T12:03:36.039339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_past_answer = [frame['user_prev_answer_lag'].to_numpy()[:, None].tolist()[i:i+max_len] for _, frame in train_group\n",
    "                         for i in range(0, len(frame['user_prev_answer_lag'].to_numpy()[:,None]), max_len )]\n",
    "\n",
    "train_past_answer = np.reshape(pad_sequences(train_past_answer, padding=\"pre\"),(-1,max_len))\n",
    "\n",
    "\n",
    "f = open(\"train_past_answer.pkl\",\"wb\")\n",
    "pickle.dump(train_past_answer,f)\n",
    "f.close()\n",
    "\n",
    "del(train_past_answer)\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:10:22.881441Z",
     "iopub.status.busy": "2020-12-15T12:10:22.880738Z",
     "iopub.status.idle": "2020-12-15T12:17:37.424089Z",
     "shell.execute_reply": "2020-12-15T12:17:37.423226Z"
    },
    "papermill": {
     "duration": 434.57317,
     "end_time": "2020-12-15T12:17:37.424302",
     "exception": false,
     "start_time": "2020-12-15T12:10:22.851132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_prior_exp = [frame['prior_question_had_explanation'].to_numpy()[:, None].tolist()[i:i+max_len] for _, frame in train_group\n",
    "                         for i in range(0, len(frame['prior_question_had_explanation'].to_numpy()[:,None]), max_len )]\n",
    "\n",
    "train_prior_exp = np.reshape(pad_sequences(train_prior_exp, padding=\"pre\"),(-1,max_len))\n",
    "\n",
    "f = open(\"train_prior_exp.pkl\",\"wb\")\n",
    "pickle.dump(train_prior_exp,f)\n",
    "f.close()\n",
    "\n",
    "del(train_prior_exp)\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:17:37.486226Z",
     "iopub.status.busy": "2020-12-15T12:17:37.485222Z",
     "iopub.status.idle": "2020-12-15T12:50:43.568131Z",
     "shell.execute_reply": "2020-12-15T12:50:43.567321Z"
    },
    "papermill": {
     "duration": 1986.114456,
     "end_time": "2020-12-15T12:50:43.568294",
     "exception": false,
     "start_time": "2020-12-15T12:17:37.453838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_other_feats = [frame[FEATS].to_numpy()[:, None].tolist()[i:i+max_len] for _, frame in train_group\n",
    "                         for i in range(0, len(frame[FEATS].to_numpy()[:,None]), max_len )]\n",
    "\n",
    "train_other_feats = np.reshape(pad_sequences(train_other_feats, padding=\"pre\", dtype = 'float32'),(-1,max_len, len(FEATS)))\n",
    "\n",
    "f = open(\"train_other_feats.pkl\",\"wb\")\n",
    "pickle.dump(train_other_feats,f)\n",
    "f.close()\n",
    "\n",
    "del(train_other_feats)\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:50:43.650086Z",
     "iopub.status.busy": "2020-12-15T12:50:43.649046Z",
     "iopub.status.idle": "2020-12-15T12:53:03.039215Z",
     "shell.execute_reply": "2020-12-15T12:53:03.037778Z"
    },
    "papermill": {
     "duration": 139.437672,
     "end_time": "2020-12-15T12:53:03.039383",
     "exception": false,
     "start_time": "2020-12-15T12:50:43.601711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_group = valid.groupby('user_id')\n",
    "\n",
    "valid_y = [frame['answered_correctly'].to_numpy()[:, None].tolist()[i:i+max_len] for _, frame in valid_group\n",
    "           for i in range(0, len(frame['answered_correctly'].to_numpy()[:, None]), max_len) ]\n",
    "valid_current_question = [frame['content_id'].to_numpy()[:, None].tolist()[i:i+max_len] for _, frame in valid_group\n",
    "                         for i in range(0, len(frame['content_id'].to_numpy()[:,None]), max_len )]\n",
    "valid_current_tag = [frame['tag_num'].to_numpy()[:, None].tolist()[i:i+max_len] for _, frame in valid_group\n",
    "                         for i in range(0, len(frame['tag_num'].to_numpy()[:,None]), max_len )] \n",
    "valid_current_part = [frame['part'].to_numpy()[:, None].tolist()[i:i+max_len] for _, frame in valid_group\n",
    "                         for i in range(0, len(frame['part'].to_numpy()[:,None]), max_len )] \n",
    "\n",
    "valid_past_answer = [frame['user_prev_answer_lag'].to_numpy()[:, None].tolist()[i:i+max_len] for _, frame in valid_group\n",
    "                         for i in range(0, len(frame['user_prev_answer_lag'].to_numpy()[:,None]), max_len )]\n",
    "valid_other_feats = [frame[FEATS].to_numpy()[:, None].tolist()[i:i+max_len] for _, frame in valid_group\n",
    "                         for i in range(0, len(frame[FEATS].to_numpy()[:,None]), max_len )]\n",
    "valid_prior_exp = [frame['prior_question_had_explanation'].to_numpy()[:, None].tolist()[i:i+max_len] for _, frame in valid_group\n",
    "                         for i in range(0, len(frame['prior_question_had_explanation'].to_numpy()[:,None]), max_len )]\n",
    "\n",
    "\n",
    "valid_y = np.reshape(pad_sequences(valid_y, padding=\"pre\"),(-1,max_len,1))\n",
    "valid_current_question = np.reshape(pad_sequences(valid_current_question, padding=\"pre\"),(-1,max_len))\n",
    "valid_current_tag = np.reshape(pad_sequences(valid_current_tag, padding=\"pre\"),(-1,max_len))\n",
    "valid_current_part = np.reshape(pad_sequences(valid_current_part, padding=\"pre\"),(-1,max_len))\n",
    "valid_past_answer = np.reshape(pad_sequences(valid_past_answer, padding=\"pre\"),(-1,max_len))\n",
    "valid_other_feats = np.reshape(pad_sequences(valid_other_feats, padding=\"pre\", dtype = 'float32'),(-1,max_len, len(FEATS)))\n",
    "valid_prior_exp = np.reshape(pad_sequences(valid_prior_exp, padding=\"pre\"),(-1,max_len))\n",
    "\n",
    "\n",
    "f = open(\"valid_prior_exp.pkl\",\"wb\")\n",
    "pickle.dump(valid_prior_exp,f)\n",
    "f.close()\n",
    "del(valid_prior_exp)\n",
    "_ = gc.collect()\n",
    "\n",
    "f = open(\"valid_y.pkl\",\"wb\")\n",
    "pickle.dump(valid_y,f)\n",
    "f.close()\n",
    "del(valid_y)\n",
    "_ = gc.collect()\n",
    "\n",
    "f = open(\"valid_current_question.pkl\",\"wb\")\n",
    "pickle.dump(valid_current_question,f)\n",
    "f.close()\n",
    "del(valid_current_question)\n",
    "_ = gc.collect()\n",
    "\n",
    "f = open(\"valid_current_tag.pkl\",\"wb\")\n",
    "pickle.dump(valid_current_tag,f)\n",
    "f.close()\n",
    "del(valid_current_tag)\n",
    "_ = gc.collect()\n",
    "\n",
    "f = open(\"valid_current_part.pkl\",\"wb\")\n",
    "pickle.dump(valid_current_part,f)\n",
    "f.close()\n",
    "del(valid_current_part)\n",
    "_ = gc.collect()\n",
    "\n",
    "f = open(\"valid_past_answer.pkl\",\"wb\")\n",
    "pickle.dump(valid_past_answer,f)\n",
    "f.close()\n",
    "del(valid_past_answer)\n",
    "_ = gc.collect()\n",
    "\n",
    "f = open(\"valid_other_feats.pkl\",\"wb\")\n",
    "pickle.dump(valid_other_feats,f)\n",
    "f.close()\n",
    "del(valid_other_feats)\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:53:03.094898Z",
     "iopub.status.busy": "2020-12-15T12:53:03.094022Z",
     "iopub.status.idle": "2020-12-15T12:53:03.097032Z",
     "shell.execute_reply": "2020-12-15T12:53:03.097440Z"
    },
    "papermill": {
     "duration": 0.034945,
     "end_time": "2020-12-15T12:53:03.097587",
     "exception": false,
     "start_time": "2020-12-15T12:53:03.062642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, LSTM, Masking, Embedding, Concatenate, Input, Reshape,Flatten, AveragePooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Lambda\n",
    "#from tensorflow.keras.layers import merge\n",
    "from tensorflow.keras.layers import multiply, Reshape\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023069,
     "end_time": "2020-12-15T12:53:03.143349",
     "exception": false,
     "start_time": "2020-12-15T12:53:03.120280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:53:03.194399Z",
     "iopub.status.busy": "2020-12-15T12:53:03.193124Z",
     "iopub.status.idle": "2020-12-15T12:53:03.250734Z",
     "shell.execute_reply": "2020-12-15T12:53:03.250145Z"
    },
    "papermill": {
     "duration": 0.084329,
     "end_time": "2020-12-15T12:53:03.250862",
     "exception": false,
     "start_time": "2020-12-15T12:53:03.166533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "      q: query shape == (..., seq_len_q, depth)\n",
    "      k: key shape == (..., seq_len_k, depth)\n",
    "      v: value shape == (..., seq_len_v, depth_v)\n",
    "      mask: Float tensor with shape broadcastable \n",
    "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "      output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                       (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class EncoderLayer2(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.2):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "\n",
    "class Encoder2(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                   maximum_position_encoding, rate=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        #x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    \n",
    "class DecoderLayer2(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.2):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, enc_output, training, \n",
    "                look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "                enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "    \n",
    "    \n",
    "class Decoder2(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                    maximum_position_encoding, rate=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, \n",
    "               look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        #x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "              x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "        attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "        attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights    \n",
    "\n",
    "class Transformer2(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, padding_length, rate=0.2):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder2(num_layers, d_model, num_heads, dff, padding_length)\n",
    "        \n",
    "        self.decoder = Decoder2(num_layers, d_model, num_heads, dff, padding_length)\n",
    "\n",
    "        self.second_final_layer = tf.keras.layers.Dense(dff)\n",
    "        self.final_layer = Dense(1,activation = 'sigmoid')\n",
    "    \n",
    "    def call(self, inp1, inp2, training, en_combined_mask, de_look_ahead_mask, de_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp1, training, en_combined_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "                inp2, enc_output, training, de_look_ahead_mask, de_padding_mask)\n",
    "            \n",
    "        second_final_output = self.second_final_layer(dec_output)  # (batch_size, tar_seq_len, question_answer_pair_size)\n",
    "        final_output = self.final_layer(second_final_output)\n",
    "        return final_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:53:03.315843Z",
     "iopub.status.busy": "2020-12-15T12:53:03.314933Z",
     "iopub.status.idle": "2020-12-15T12:53:03.317952Z",
     "shell.execute_reply": "2020-12-15T12:53:03.317490Z"
    },
    "papermill": {
     "duration": 0.043977,
     "end_time": "2020-12-15T12:53:03.318068",
     "exception": false,
     "start_time": "2020-12-15T12:53:03.274091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "num_other_feats = len(FEATS)\n",
    "num_layers = 2\n",
    "d_model = 256\n",
    "num_heads = 4\n",
    "dff = 128\n",
    "\n",
    "n_question = 13523\n",
    "n_tag = 1520\n",
    "n_part = 7\n",
    "n_answer = 3\n",
    "\n",
    "pe_input = 100\n",
    "epoch = 10\n",
    "max_len = 100\n",
    "\n",
    "\n",
    "def build(num_layers, d_model, num_heads, dff, n_question, n_tag, n_part, n_answer, pe_input, num_other_feats, max_len):\n",
    "\n",
    "    en_input1 = Input(batch_shape = (None, None), name = 'current_question')\n",
    "    en_input1_embed = Embedding(n_question, d_model)(en_input1)\n",
    "    en_input2 = Input(batch_shape = (None, None), name = 'current_tag')\n",
    "    en_input2_embed = Embedding(n_tag, d_model)(en_input2)\n",
    "    en_input3 = Input(batch_shape = (None, None), name = 'current_part')\n",
    "    en_input3_embed = Embedding(n_part, d_model)(en_input3)\n",
    "    en_input = tf.math.add_n([en_input1_embed, en_input2_embed, en_input3_embed])\n",
    "\n",
    "    en_look_ahead_mask = create_look_ahead_mask(tf.shape(en_input1)[1])\n",
    "    en_padding_mask = create_padding_mask(en_input1)\n",
    "    en_combined_mask = tf.maximum(look_ahead_mask, padding_mask)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #en_input1_embed = K.sum(en_input1_embed, axis = -2)\n",
    "    de_input4 = Input(batch_shape = (None, None), name = 'past_answer')\n",
    "    de_input4_embed = Embedding(n_answer, d_model)(en_input4)\n",
    "\n",
    "    de_input5 = Input(batch_shape = (None, None, num_other_feats), name = 'other_feature')\n",
    "    de_input5_masked = (Masking(mask_value= 0, input_shape = (None, None, num_other_feats)))(en_input5)\n",
    "    de_input5_embed = Dense(d_model, input_shape = (None, None, num_other_feats), activation = 'sigmoid')(en_input5_masked)    \n",
    "    de_input = tf.math.add_n([de_input4_embed, de_input5_embed])\n",
    "    \n",
    "    de_look_ahead_mask = create_look_ahead_mask(tf.shape(de_input4)[1])\n",
    "    de_padding_mask = create_padding_mask(de_input4)\n",
    "\n",
    "    \n",
    "    \n",
    "    transformer = Transformer2(num_layers, d_model, num_heads, dff, pe_input)\n",
    "    \n",
    "    final_output = transformer(en_input, de_input, True, en_combined_mask, de_look_ahead_mask, de_padding_mask)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[en_input1, en_input2, en_input3, de_input4, de_input5], outputs=final_output)\n",
    "    model.compile( optimizer = 'adam',\n",
    "                    loss = 'binary_crossentropy',\n",
    "                    metrics=['accuracy',AUC()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#my_model = build(num_layers, d_model, num_heads, dff, n_question, n_tag, n_part, n_answer, pe_input, num_other_feats, max_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:53:03.374594Z",
     "iopub.status.busy": "2020-12-15T12:53:03.373555Z",
     "iopub.status.idle": "2020-12-15T12:53:03.376967Z",
     "shell.execute_reply": "2020-12-15T12:53:03.377468Z"
    },
    "papermill": {
     "duration": 0.036485,
     "end_time": "2020-12-15T12:53:03.377617",
     "exception": false,
     "start_time": "2020-12-15T12:53:03.341132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#my_model.fit([train_current_question, train_current_tag, train_current_part, train_past_answer, train_other_feats] ,train_y, \n",
    "#             validation_data=([valid_current_question, valid_current_tag, valid_current_part, valid_past_answer, valid_other_feats] ,valid_y), batch_size = 200,  epochs = 15, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:53:03.430441Z",
     "iopub.status.busy": "2020-12-15T12:53:03.429754Z",
     "iopub.status.idle": "2020-12-15T12:53:03.722053Z",
     "shell.execute_reply": "2020-12-15T12:53:03.721464Z"
    },
    "papermill": {
     "duration": 0.32126,
     "end_time": "2020-12-15T12:53:03.722170",
     "exception": false,
     "start_time": "2020-12-15T12:53:03.400910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c3f18cd2d5d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SAINT_model_feature_extraction.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'my_model' is not defined"
     ]
    }
   ],
   "source": [
    "my_model.save_weights('SAINT_model_feature_extraction.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024707,
     "end_time": "2020-12-15T12:53:03.771173",
     "exception": false,
     "start_time": "2020-12-15T12:53:03.746466",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Have a fun with loops! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 4899.130243,
   "end_time": "2020-12-15T12:53:03.909950",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-15T11:31:24.779707",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
